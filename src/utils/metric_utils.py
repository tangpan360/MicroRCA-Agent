import pandas as pd
import os
import sys
from typing import Optional, List, Tuple, Dict
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥utils.io_util
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
import utils.io_util as io

# å¯¼å…¥å¤§æ¨¡å‹ç›¸å…³æ¨¡å—
from agent.agents import create_llm


def get_fault_period_info(df_fault_timestamps: pd.DataFrame, row_index: int) -> Tuple[List[str], str, str, str]:
    """
    è·å–æŒ‡å®šè¡Œçš„æ•…éšœæ—¶é—´æ®µä¿¡æ¯

    å‚æ•°:
        df_fault_timestamps: åŒ…å«æ•…éšœèµ·æ­¢æ—¶é—´æˆ³çš„DataFrame
        row_index: æŒ‡å®šè¦æŸ¥è¯¢çš„è¡Œç´¢å¼•

    è¿”å›:
        åŒ¹é…çš„Podæ–‡ä»¶åˆ—è¡¨, æ—¥æœŸ, å¼€å§‹æ—¶é—´, ç»“æŸæ—¶é—´
    """
    row = df_fault_timestamps.iloc[row_index]
    date = row['date']
    start_time = row['start_timestamp']
    end_time = row['end_timestamp']

    # æ„å»ºPodæ•°æ®ç›®å½•è·¯å¾„
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    pod_dir = os.path.join(project_root, 'data', 'processed', f'{date}', 'metric-parquet', 'apm', 'pod')
    matching_files = os.listdir(pod_dir)

    return matching_files, date, start_time, end_time


def extract_service_name_from_pod(pod_name: str) -> str:
    """
    ä»podåç§°ä¸­æå–serviceåç§°

    å‚æ•°:
        pod_name: podåç§°ï¼Œå¦‚ "redis-cart-0"

    è¿”å›:
        serviceåç§°ï¼Œå¦‚ "redis"
    """
    # æå–ç”¨-åˆ†å‰²åçš„ç¬¬ä¸€é¡¹ä½œä¸ºæœåŠ¡å
    if '-' in pod_name:
        return pod_name.split('-')[0]
    return pod_name


def get_normal_time_periods(df_fault_timestamps: pd.DataFrame, current_index: int) -> List[Tuple[str, str]]:
    """
    è·å–æ­£å¸¸æ—¶é—´æ®µï¼ˆå½“å‰æ•…éšœå‰åçš„æ­£å¸¸æ—¶é—´æ®µï¼‰

    å‚æ•°:
        df_fault_timestamps: æ•…éšœæ—¶é—´æˆ³DataFrame
        current_index: å½“å‰æ•…éšœç´¢å¼•

    è¿”å›:
        æ­£å¸¸æ—¶é—´æ®µåˆ—è¡¨ [(start_time, end_time), ...]
    """
    normal_periods = []
    current_row = df_fault_timestamps.iloc[current_index]
    current_start = current_row['start_timestamp']
    current_end = current_row['end_timestamp']

    # è·å–å½“å‰æ•…éšœå‰çš„æ­£å¸¸æ—¶é—´æ®µï¼ˆä¸Šä¸€ä¸ªæ•…éšœç»“æŸåˆ°å½“å‰æ•…éšœå¼€å§‹ï¼‰
    if current_index > 0:
        prev_row = df_fault_timestamps.iloc[current_index - 1]
        prev_end = prev_row['end_timestamp']
        # æ­£å¸¸æ—¶é—´æ®µï¼šä¸Šä¸€ä¸ªæ•…éšœç»“æŸå10åˆ†é’Ÿ åˆ° å½“å‰æ•…éšœå¼€å§‹
        normal_periods.append((prev_end + 10 * 60 * 1_000_000_000, current_start))
        # normal_periods.append((prev_end , current_start))

    # è·å–å½“å‰æ•…éšœåçš„æ­£å¸¸æ—¶é—´æ®µï¼ˆå½“å‰æ•…éšœç»“æŸåˆ°ä¸‹ä¸€ä¸ªæ•…éšœå¼€å§‹ï¼‰
    if current_index < len(df_fault_timestamps) - 1:
        next_row = df_fault_timestamps.iloc[current_index + 1]
        next_start = next_row['start_timestamp']
        # æ­£å¸¸æ—¶é—´æ®µï¼šå½“å‰æ•…éšœç»“æŸ åˆ° ä¸‹ä¸€ä¸ªæ•…éšœå¼€å§‹
        normal_periods.append((current_end + 10 * 60 * 1_000_000_000, next_start))
        # normal_periods.append((current_end , next_start))

    return normal_periods


def get_metrics_description_from_dataframe(df_pod: pd.DataFrame, columns: List[str] = None) -> Dict[str, pd.Series]:
    """
    è·å–DataFrameæŒ‡å®šåˆ—çš„ç»Ÿè®¡æè¿°ä¿¡æ¯

    å‚æ•°:
        df_pod: PodæŒ‡æ ‡æ•°æ®çš„DataFrame
        columns: éœ€è¦è·å–æè¿°ç»Ÿè®¡çš„åˆ—ååˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ•°å€¼å‹åˆ—

    è¿”å›:
        åŒ…å«æ¯åˆ—æè¿°ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    if columns is None:
        # é»˜è®¤é€‰æ‹©æ•°å€¼å‹åˆ—
        numeric_columns = ['client_error_ratio', 'error_ratio', 'request', 'response', 'rrt', 'server_error_ratio',
                           'timeout']
        # è¿‡æ»¤å‡ºå®é™…å­˜åœ¨çš„åˆ—
        columns = [col for col in numeric_columns if col in df_pod.columns]

    descriptions = {}
    for column in columns:
        if column in df_pod.columns:
            # æè¿°ç»Ÿè®¡ï¼ˆå« 0.25ã€0.5ã€0.75ã€0.95ã€0.99ï¼‰
            desc = df_pod[column].describe(percentiles=[0.25, 0.5, 0.75, 0.95, 0.99])

            # è®¡ç®—éé›¶æ¯”ä¾‹
            col_data = df_pod[column].dropna()
            non_zero_ratio = (col_data != 0).sum() / len(col_data) if len(col_data) > 0 else 0
            desc['non_zero_ratio'] = round(non_zero_ratio, 3)  # ä¿ç•™ä¸‰ä½å°æ•°

            descriptions[column] = desc
        else:
            print(f"è­¦å‘Š: åˆ— '{column}' ä¸å­˜åœ¨äºDataFrameä¸­")

    return descriptions


def get_filtered_metrics_description_with_outlier_removal(df_pod: pd.DataFrame, start_time: str, end_time: str,
                                                          target_columns: List[str] = None,
                                                          remove_outliers: bool = False) -> Dict[str, pd.Series]:
    """
    è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æŒ‡æ ‡æè¿°ç»Ÿè®¡ï¼Œå¯é€‰æ‹©æ˜¯å¦ç§»é™¤å¼‚å¸¸å€¼

    å‚æ•°:
        df_pod: PodæŒ‡æ ‡æ•°æ®çš„DataFrame
        start_time: å¼€å§‹æ—¶é—´æˆ³
        end_time: ç»“æŸæ—¶é—´æˆ³
        target_columns: éœ€è¦åˆ†æçš„åˆ—ååˆ—è¡¨
        remove_outliers: æ˜¯å¦ç§»é™¤å¼‚å¸¸å€¼ï¼ˆæœ€å°2ä¸ªå’Œæœ€å¤§2ä¸ªå€¼ï¼‰

    è¿”å›:
        æŒ‡æ ‡æè¿°ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    if 'timestamp_ns' in df_pod.columns:
        # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºæ•´æ•°è¿›è¡Œæ¯”è¾ƒ
        start_ts = int(start_time)
        end_ts = int(end_time)
        df_filtered = df_pod[(df_pod['timestamp_ns'] >= start_ts) & (df_pod['timestamp_ns'] <= end_ts)]
    else:
        print("è­¦å‘Š: æœªæ‰¾åˆ°timestamp_nsåˆ—ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´è¿‡æ»¤")
        df_filtered = df_pod

    if len(df_filtered) == 0:
        print("æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ— æ•°æ®")
        return {}

    # å¦‚æœéœ€è¦ç§»é™¤å¼‚å¸¸å€¼ä¸”æ•°æ®é‡è¶³å¤Ÿ
    if remove_outliers and len(df_filtered) > 4:  # è‡³å°‘éœ€è¦5ä¸ªæ•°æ®ç‚¹æ‰èƒ½ç§»é™¤4ä¸ª
        return get_metrics_description_from_dataframe_without_outliers(df_filtered, target_columns)
    else:
        return get_metrics_description_from_dataframe(df_filtered, target_columns)


def get_metrics_description_from_dataframe_without_outliers(df_pod: pd.DataFrame, columns: List[str] = None) -> Dict[
    str, pd.Series]:
    """
    è·å–DataFrameæŒ‡å®šåˆ—çš„ç»Ÿè®¡æè¿°ä¿¡æ¯ï¼Œç§»é™¤æœ€å°2ä¸ªå’Œæœ€å¤§2ä¸ªå€¼

    å‚æ•°:
        df_pod: PodæŒ‡æ ‡æ•°æ®çš„DataFrame
        columns: éœ€è¦è·å–æè¿°ç»Ÿè®¡çš„åˆ—ååˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ•°å€¼å‹åˆ—

    è¿”å›:
        åŒ…å«æ¯åˆ—æè¿°ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    if columns is None:
        # é»˜è®¤é€‰æ‹©æ•°å€¼å‹åˆ—
        numeric_columns = ['client_error_ratio', 'error_ratio', 'request', 'response', 'rrt', 'server_error_ratio',
                           'timeout']
        # è¿‡æ»¤å‡ºå®é™…å­˜åœ¨çš„åˆ—
        columns = [col for col in numeric_columns if col in df_pod.columns]

    descriptions = {}
    for column in columns:
        if column in df_pod.columns:
            # è·å–è¯¥åˆ—çš„æ•°æ®å¹¶æ’åº
            col_data = df_pod[column].dropna().sort_values()

            if len(col_data) <= 4:
                # æ•°æ®ç‚¹å¤ªå°‘ï¼Œç›´æ¥ç”¨åŸå§‹æ•°æ®æè¿°
                desc = col_data.describe(percentiles=[0.25, 0.5, 0.75, 0.95, 0.99])
                print(f"è­¦å‘Š: åˆ— '{column}' æ•°æ®ç‚¹ä¸è¶³({len(col_data)}ä¸ª)ï¼Œæœªç§»é™¤å¼‚å¸¸å€¼")
            else:
                # å»æ‰æœ€å°2ä¸ªå’Œæœ€å¤§2ä¸ª
                trimmed_data = col_data.iloc[2:-2]
                desc = trimmed_data.describe(percentiles=[0.25, 0.5, 0.75, 0.95, 0.99])
                print(f"åˆ— '{column}': åŸå§‹æ•°æ®{len(col_data)}ä¸ªï¼Œç§»é™¤æœ€å¤§å’Œæœ€å°ä¸¤ä¸ªå€¼å{len(trimmed_data)}ä¸ª")

            # è®¡ç®—éé›¶æ¯”ä¾‹ï¼ˆåŸºäºå»é™¤å¼‚å¸¸å€¼åçš„æ•°æ®ï¼‰
            non_zero_ratio = (trimmed_data != 0).sum() / len(trimmed_data) if len(col_data) > 4 else (col_data != 0).sum() / len(col_data)
            desc['non_zero_ratio'] = round(non_zero_ratio, 3)

            descriptions[column] = desc
        else:
            print(f"è­¦å‘Š: åˆ— '{column}' ä¸å­˜åœ¨äºDataFrameä¸­")

    return descriptions


def analyze_fault_vs_normal_metrics_by_service(df_fault_timestamps: pd.DataFrame, index: int,
                                               target_columns: List[str] = None) -> Optional[Dict]:
    """
    æŒ‰Serviceçº§åˆ«åˆ†ææ•…éšœæ—¶é—´æ®µä¸æ­£å¸¸æ—¶é—´æ®µçš„æŒ‡æ ‡å¯¹æ¯”
    ç»“æ„ï¼šservice â†’ pod â†’ metrics (normal_periods_combined, fault_period)

    å‚æ•°:
        df_fault_timestamps: æ•…éšœæ—¶é—´æˆ³DataFrame
        index: è¦åˆ†æçš„æ•…éšœç´¢å¼•
        target_columns: éœ€è¦åˆ†æçš„æŒ‡æ ‡åˆ—ååˆ—è¡¨

    è¿”å›:
        æŒ‰Serviceç»„ç»‡çš„åŒ…å«æ•…éšœå’Œæ­£å¸¸æ—¶é—´æ®µæŒ‡æ ‡å¯¹æ¯”çš„å­—å…¸
    """
    pod_files, date, fault_start, fault_end = get_fault_period_info(df_fault_timestamps, index)

    if not pod_files:
        print("æœªæ‰¾åˆ°åŒ¹é…çš„Podæ–‡ä»¶")
        return None

    normal_periods = get_normal_time_periods(df_fault_timestamps, index)

    print(f"æ•…éšœæ—¥æœŸ: {date}")
    print(f"æ•…éšœæ—¶é—´æ®µ: {fault_start} è‡³ {fault_end}")
    print(f"æ­£å¸¸æ—¶é—´æ®µæ•°é‡: {len(normal_periods)}")
    print(f"åŒ¹é…çš„Podæ–‡ä»¶æ•°é‡: {len(pod_files)}")

    # æŒ‰Service â†’ Pod â†’ Metrics ç»“æ„ç»„ç»‡åˆ†æç»“æœ
    service_analysis = {}

    for pod_file in pod_files:
        pod_path = os.path.join(project_root, 'data', 'processed', f'{date}', 'metric-parquet', 'apm', 'pod', pod_file)
        pod_name = pod_file.split('_')[1] if '_' in pod_file else pod_file.split('.')[0]
        service_name = extract_service_name_from_pod(pod_name)

        try:
            df_pod = pd.read_parquet(pod_path)

            if len(df_pod) == 0:
                print(f"Pod {pod_name} æ— æ•°æ®")
                continue

            # å¦‚æœserviceä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–
            if service_name not in service_analysis:
                service_analysis[service_name] = {}

            # å¦‚æœpodä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–
            if pod_name not in service_analysis[service_name]:
                service_analysis[service_name][pod_name] = {
                    'normal_periods_combined': {},  # åˆå¹¶çš„æ­£å¸¸æ•°æ®ç»Ÿè®¡
                    'fault_period': {}  # æ•…éšœæ•°æ®ç»Ÿè®¡
                }

            print(f"\n=== Service: {service_name} | Pod: {pod_name} ===")

            # 1. å…ˆåˆå¹¶æ‰€æœ‰æ­£å¸¸æ—¶é—´æ®µçš„æ•°æ®è¿›è¡Œç»Ÿè®¡
            print(f"\nğŸ“ˆ æ­£å¸¸æ—¶é—´æ®µåˆå¹¶åˆ†æï¼ˆå·²ç§»é™¤å¼‚å¸¸å€¼ï¼‰:")

            # æ”¶é›†æ‰€æœ‰æ­£å¸¸æ—¶é—´æ®µçš„æ•°æ®
            all_normal_data = []
            total_normal_count = 0

            for i, (normal_start, normal_end) in enumerate(normal_periods):
                print(f"  åŒ…å«æ­£å¸¸æ—¶é—´æ®µ {i + 1}: {normal_start} è‡³ {normal_end}")

                # è¿‡æ»¤å½“å‰æ­£å¸¸æ—¶é—´æ®µçš„æ•°æ®
                start_ts = int(normal_start)
                end_ts = int(normal_end)
                normal_data = df_pod[(df_pod['timestamp_ns'] >= start_ts) & (df_pod['timestamp_ns'] <= end_ts)]

                if len(normal_data) > 0:
                    all_normal_data.append(normal_data)
                    total_normal_count += len(normal_data)
                    print(f"    æ—¶é—´æ®µ {i + 1} æ•°æ®è¡Œæ•°: {len(normal_data)}")

            # åˆå¹¶æ‰€æœ‰æ­£å¸¸æ—¶é—´æ®µçš„æ•°æ®
            if all_normal_data:
                combined_normal_data = pd.concat(all_normal_data, ignore_index=True)
                print(f"  åˆå¹¶åæ­£å¸¸æ—¶é—´æ®µæ€»æ•°æ®è¡Œæ•°: {len(combined_normal_data)}")

                # å¯¹åˆå¹¶çš„æ­£å¸¸æ•°æ®è¿›è¡Œç»Ÿè®¡ï¼ˆç§»é™¤å¼‚å¸¸å€¼ï¼‰
                if len(combined_normal_data) > 4:  # è‡³å°‘éœ€è¦5ä¸ªæ•°æ®ç‚¹æ‰èƒ½ç§»é™¤4ä¸ª
                    normal_desc = get_metrics_description_from_dataframe_without_outliers(combined_normal_data,
                                                                                          target_columns)
                else:
                    normal_desc = get_metrics_description_from_dataframe(combined_normal_data, target_columns)

                service_analysis[service_name][pod_name]['normal_periods_combined'] = normal_desc

                if normal_desc:
                    print("  åˆå¹¶æ­£å¸¸æœŸé—´æŒ‡æ ‡ç»Ÿè®¡:")
                    for col_name, desc in normal_desc.items():
                        print(f"    {col_name}: mean={desc['mean']:.2f}, std={desc['std']:.2f}")
            else:
                print("  æœªæ‰¾åˆ°æ­£å¸¸æ—¶é—´æ®µæ•°æ®")

            # 2. å†è·å–æ•…éšœæ—¶é—´æ®µçš„ç»Ÿè®¡ï¼ˆä¸ç§»é™¤å¼‚å¸¸å€¼ï¼‰
            print(f"\nğŸ“Š æ•…éšœæ—¶é—´æ®µåˆ†æ:")
            fault_desc = get_filtered_metrics_description_with_outlier_removal(
                df_pod, fault_start, fault_end, target_columns, remove_outliers=False
            )

            service_analysis[service_name][pod_name]['fault_period'] = fault_desc

            fault_data_count = len(df_pod[(df_pod['timestamp_ns'] >= int(fault_start)) &
                                          (df_pod['timestamp_ns'] <= int(fault_end))])
            print(f"  æ•…éšœæ—¶é—´æ®µæ•°æ®è¡Œæ•°: {fault_data_count}")

            if fault_desc:
                print("  æ•…éšœæœŸé—´æŒ‡æ ‡ç»Ÿè®¡:")
                for col_name, desc in fault_desc.items():
                    print(f"    {col_name}: mean={desc['mean']:.2f}, std={desc['std']:.2f}")

        except Exception as e:
            print(f"å¤„ç†Podæ–‡ä»¶ {pod_file} æ—¶å‡ºé”™: {e}")

    return service_analysis if service_analysis else None


def get_node_metrics_files_mapping(date: str) -> Dict[str, str]:
    """
    è·å–èŠ‚ç‚¹æŒ‡æ ‡æ–‡ä»¶åæ˜ å°„ï¼Œè¿”å›æŒ‡æ ‡åç§°åˆ°æ–‡ä»¶åçš„æ˜ å°„å…³ç³»

    å‚æ•°:
        date: æ—¥æœŸï¼Œæ ¼å¼å¦‚ "2025-06-06"

    è¿”å›:
        æŒ‡æ ‡ååˆ°æ–‡ä»¶åçš„æ˜ å°„å­—å…¸
    """
    return {
        'node_cpu_usage_rate': f'infra_node_node_cpu_usage_rate_{date}.parquet',
        'node_disk_read_bytes_total': f'infra_node_node_disk_read_bytes_total_{date}.parquet',
        'node_disk_read_time_seconds_total': f'infra_node_node_disk_read_time_seconds_total_{date}.parquet',
        'node_disk_write_time_seconds_total': f'infra_node_node_disk_write_time_seconds_total_{date}.parquet',
        'node_disk_written_bytes_total': f'infra_node_node_disk_written_bytes_total_{date}.parquet',
        'node_filesystem_free_bytes': f'infra_node_node_filesystem_free_bytes_{date}.parquet',
        'node_filesystem_size_bytes': f'infra_node_node_filesystem_size_bytes_{date}.parquet',
        'node_filesystem_usage_rate': f'infra_node_node_filesystem_usage_rate_{date}.parquet',
        'node_memory_MemAvailable_bytes': f'infra_node_node_memory_MemAvailable_bytes_{date}.parquet',
        'node_memory_MemTotal_bytes': f'infra_node_node_memory_MemTotal_bytes_{date}.parquet',
        'node_memory_usage_rate': f'infra_node_node_memory_usage_rate_{date}.parquet',
        'node_network_receive_bytes_total': f'infra_node_node_network_receive_bytes_total_{date}.parquet',
        'node_network_receive_packets_total': f'infra_node_node_network_receive_packets_total_{date}.parquet',
        'node_network_transmit_bytes_total': f'infra_node_node_network_transmit_bytes_total_{date}.parquet',
        'node_network_transmit_packets_total': f'infra_node_node_network_transmit_packets_total_{date}.parquet',
        'node_sockstat_TCP_inuse': f'infra_node_node_sockstat_TCP_inuse_{date}.parquet'
    }


def get_target_nodes() -> List[str]:
    """
    è·å–ç›®æ ‡åˆ†æèŠ‚ç‚¹åˆ—è¡¨ï¼ˆåªåˆ†æaiops-k8s-01åˆ°aiops-k8s-08è¿™8ä¸ªèŠ‚ç‚¹ï¼‰

    è¿”å›:
        ç›®æ ‡èŠ‚ç‚¹åç§°åˆ—è¡¨
    """
    return [f'aiops-k8s-{i:02d}' for i in range(1, 9)]  # aiops-k8s-01 åˆ° aiops-k8s-08


def load_node_metric_data(date: str, metric_name: str) -> Optional[pd.DataFrame]:
    """
    åŠ è½½æŒ‡å®šæ—¥æœŸå’ŒæŒ‡æ ‡çš„èŠ‚ç‚¹æ•°æ®

    å‚æ•°:
        date: æ—¥æœŸï¼Œæ ¼å¼å¦‚ "2025-06-06"
        metric_name: æŒ‡æ ‡åç§°ï¼Œå¦‚ "node_cpu_usage_rate"

    è¿”å›:
        èŠ‚ç‚¹æŒ‡æ ‡æ•°æ®DataFrameï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›None
    """
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    node_dir = os.path.join(project_root, 'data', 'processed', f'{date}', 'metric-parquet', 'infra', 'infra_node')

    file_mapping = get_node_metrics_files_mapping(date)

    if metric_name not in file_mapping:
        print(f"æ•…éšœçš„æŒ‡æ ‡åç§°: {metric_name}")
        return None

    file_path = os.path.join(node_dir, file_mapping[metric_name])

    try:
        if not os.path.exists(file_path):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None

        df = pd.read_parquet(file_path)

        # åªä¿ç•™ç›®æ ‡èŠ‚ç‚¹æ•°æ®
        target_nodes = get_target_nodes()
        df_filtered = df[df['kubernetes_node'].isin(target_nodes)]

        if len(df_filtered) == 0:
            print(f"æ–‡ä»¶ {file_path} ä¸­æœªæ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹æ•°æ®")
            return None

        return df_filtered

    except Exception as e:
        print(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        return None


def get_node_metrics_description_with_time_filter(df_node: pd.DataFrame, start_time: str, end_time: str,
                                                  metric_column: str, remove_outliers: bool = False) -> Optional[
    pd.Series]:
    """
    è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…èŠ‚ç‚¹æŒ‡æ ‡çš„æè¿°ç»Ÿè®¡

    å‚æ•°:
        df_node: èŠ‚ç‚¹æŒ‡æ ‡æ•°æ®DataFrame
        start_time: å¼€å§‹æ—¶é—´æˆ³
        end_time: ç»“æŸæ—¶é—´æˆ³
        metric_column: æŒ‡æ ‡åˆ—åï¼ˆå®é™…æ•°å€¼åˆ—ï¼‰
        remove_outliers: æ˜¯å¦ç§»é™¤å¼‚å¸¸å€¼

    è¿”å›:
        æŒ‡æ ‡æè¿°ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚æœæ— æ•°æ®åˆ™è¿”å›None
    """
    if 'timestamp_ns' not in df_node.columns:
        print("è­¦å‘Š: æœªæ‰¾åˆ°timestamp_nsåˆ—ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´è¿‡æ»¤")
        return None

    # æ—¶é—´è¿‡æ»¤
    start_ts = int(start_time)
    end_ts = int(end_time)
    df_filtered = df_node[(df_node['timestamp_ns'] >= start_ts) & (df_node['timestamp_ns'] <= end_ts)]

    if len(df_filtered) == 0:
        print("æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ— æ•°æ®")
        return None

    # è·å–æŒ‡æ ‡æ•°æ®
    if metric_column not in df_filtered.columns:
        print(f"è­¦å‘Š: åˆ— '{metric_column}' ä¸å­˜åœ¨äºDataFrameä¸­")
        return None

    metric_data = df_filtered[metric_column].dropna()

    if len(metric_data) == 0:
        print(f"æŒ‡æ ‡ '{metric_column}' æ— æœ‰æ•ˆæ•°æ®")
        return None

    # æ˜¯å¦ç§»é™¤å¼‚å¸¸å€¼
    if remove_outliers and len(metric_data) > 4:
        metric_data_sorted = metric_data.sort_values()
        metric_data = metric_data_sorted.iloc[2:-2]  # å»æ‰æœ€å°2ä¸ªå’Œæœ€å¤§2ä¸ª
        print(f"ç§»é™¤å¼‚å¸¸å€¼åæ•°æ®ç‚¹æ•°é‡: {len(metric_data)}")
     # æè¿°ç»Ÿè®¡ + ç™¾åˆ†ä½
    desc = metric_data.describe(percentiles=[0.25, 0.5, 0.75, 0.95, 0.99])

    # **æ–°å¢ï¼šéé›¶æ¯”ä¾‹**
    non_zero_ratio = (metric_data != 0).sum() / len(metric_data)
    desc['non_zero_ratio'] = round(non_zero_ratio, 3)

    return desc


def analyze_node_metrics_by_node(df_fault_timestamps: pd.DataFrame, index: int,
                                 target_metrics: List[str] = None) -> Optional[Dict]:
    """
    åˆ†ææŒ‡å®šæ•…éšœæ—¶é—´æ®µä¸æ­£å¸¸æ—¶é—´æ®µçš„èŠ‚ç‚¹æŒ‡æ ‡å¯¹æ¯”
    ç»“æ„ï¼šnode â†’ metric â†’ {normal_periods_combined, fault_period}

    å‚æ•°:
        df_fault_timestamps: æ•…éšœæ—¶é—´æˆ³DataFrame
        index: è¦åˆ†æçš„æ•…éšœç´¢å¼•
        target_metrics: éœ€è¦åˆ†æçš„æŒ‡æ ‡åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨éƒ¨10ä¸ªæŒ‡æ ‡

    è¿”å›:
        æŒ‰èŠ‚ç‚¹ç»„ç»‡çš„åŒ…å«æ•…éšœå’Œæ­£å¸¸æ—¶é—´æ®µæŒ‡æ ‡å¯¹æ¯”çš„å­—å…¸
    """
    if target_metrics is None:
        target_metrics = ['node_cpu_usage_rate',
                          'node_disk_read_bytes_total',
                          'node_disk_read_time_seconds_total',
                          'node_disk_write_time_seconds_total',
                          'node_disk_written_bytes_total',
                          'node_filesystem_free_bytes',
                          'node_filesystem_usage_rate',
                          'node_filesystem_usage_rate',
                          'node_memory_MemAvailable_bytes',
                          'node_memory_MemTotal_bytes',
                          'node_memory_usage_rate',
                          'node_network_receive_bytes_total',
                          'node_network_receive_packets_total',
                          'node_network_transmit_bytes_total',
                          'node_network_transmit_packets_total',
                          'node_sockstat_TCP_inuse', ]

    # è·å–æ•…éšœæ—¶é—´ä¿¡æ¯
    _, date, fault_start, fault_end = get_fault_period_info(df_fault_timestamps, index)
    normal_periods = get_normal_time_periods(df_fault_timestamps, index)
    target_nodes = get_target_nodes()

    print(f"èŠ‚ç‚¹åˆ†æ - æ•…éšœæ—¥æœŸ: {date}")
    print(f"èŠ‚ç‚¹åˆ†æ - æ•…éšœæ—¶é—´æ®µ: {fault_start} è‡³ {fault_end}")
    print(f"èŠ‚ç‚¹åˆ†æ - æ­£å¸¸æ—¶é—´æ®µæ•°é‡: {len(normal_periods)}")
    print(f"èŠ‚ç‚¹åˆ†æ - ç›®æ ‡èŠ‚ç‚¹æ•°é‡: {len(target_nodes)}")
    print(f"èŠ‚ç‚¹åˆ†æ - åˆ†ææŒ‡æ ‡æ•°é‡: {len(target_metrics)}")

    # æŒ‰ èŠ‚ç‚¹ â†’ æŒ‡æ ‡ â†’ æ—¶é—´æ®µ ç»“æ„ç»„ç»‡åˆ†æç»“æœ
    nodes_analysis = {}

    for node_name in target_nodes:
        print(f"\n=== å¤„ç†èŠ‚ç‚¹: {node_name} ===")

        # åˆå§‹åŒ–èŠ‚ç‚¹ç»“æ„
        nodes_analysis[node_name] = {}

        # ä¸ºå½“å‰èŠ‚ç‚¹åˆ†ææ‰€æœ‰æŒ‡æ ‡
        for metric_name in target_metrics:
            print(f"  å¤„ç†æŒ‡æ ‡: {metric_name}")

            # åŠ è½½è¯¥æŒ‡æ ‡çš„æ•°æ®
            df_metric = load_node_metric_data(date, metric_name)

            if df_metric is None:
                print(f"    æ— æ³•åŠ è½½æŒ‡æ ‡ {metric_name} çš„æ•°æ®ï¼Œè·³è¿‡")
                continue

            # è¿‡æ»¤å½“å‰èŠ‚ç‚¹çš„æ•°æ®
            df_node = df_metric[df_metric['kubernetes_node'] == node_name]

            if len(df_node) == 0:
                print(f"    èŠ‚ç‚¹ {node_name} æ—  {metric_name} æ•°æ®")
                continue

            # åˆå§‹åŒ–æŒ‡æ ‡ç»“æ„
            nodes_analysis[node_name][metric_name] = {
                'normal_periods_combined': None,
                'fault_period': None
            }

            # 1. åˆå¹¶æ‰€æœ‰æ­£å¸¸æ—¶é—´æ®µæ•°æ®è¿›è¡Œç»Ÿè®¡
            print(f"    æ­£å¸¸æ—¶é—´æ®µåˆ†æ:")
            all_normal_data = []

            for i, (normal_start, normal_end) in enumerate(normal_periods):
                start_ts = int(normal_start)
                # start_ts = int(normal_start)
                end_ts = int(normal_end)
                normal_data = df_node[(df_node['timestamp_ns'] >= start_ts) & (df_node['timestamp_ns'] <= end_ts)]

                if len(normal_data) > 0:
                    all_normal_data.append(normal_data)
                    print(f"      æ—¶é—´æ®µ {i + 1} æ•°æ®è¡Œæ•°: {len(normal_data)}")

            # åˆå¹¶æ­£å¸¸æ—¶é—´æ®µæ•°æ®å¹¶ç»Ÿè®¡
            if all_normal_data:
                combined_normal_data = pd.concat(all_normal_data, ignore_index=True)
                print(f"    åˆå¹¶åæ­£å¸¸æ—¶é—´æ®µæ€»æ•°æ®è¡Œæ•°: {len(combined_normal_data)}")

                # è·å–ç»Ÿè®¡ï¼ˆç§»é™¤å¼‚å¸¸å€¼ï¼‰
                normal_desc = get_node_metrics_description_with_time_filter(
                    combined_normal_data,
                    str(combined_normal_data['timestamp_ns'].min()),
                    str(combined_normal_data['timestamp_ns'].max()),
                    metric_name,
                    remove_outliers=(len(combined_normal_data) > 4)
                )

                nodes_analysis[node_name][metric_name]['normal_periods_combined'] = normal_desc

                if normal_desc is not None:
                    print(f"    æ­£å¸¸æœŸé—´ {metric_name}: mean={normal_desc['mean']:.2f}, std={normal_desc['std']:.2f}")

            # 2. æ•…éšœæ—¶é—´æ®µç»Ÿè®¡
            print(f"    æ•…éšœæ—¶é—´æ®µåˆ†æ:")
            fault_desc = get_node_metrics_description_with_time_filter(
                df_node, fault_start, fault_end, metric_name, remove_outliers=False
            )

            nodes_analysis[node_name][metric_name]['fault_period'] = fault_desc

            if fault_desc is not None:
                fault_data_count = len(df_node[(df_node['timestamp_ns'] >= int(fault_start)) &
                                               (df_node['timestamp_ns'] <= int(fault_end))])
                print(f"    æ•…éšœæ—¶é—´æ®µæ•°æ®è¡Œæ•°: {fault_data_count}")
                print(f"    æ•…éšœæœŸé—´ {metric_name}: mean={fault_desc['mean']:.2f}, std={fault_desc['std']:.2f}")

    return nodes_analysis if nodes_analysis else None


# ==================== 1. Pod æŒ‡æ ‡æ–‡ä»¶æ˜ å°„ ====================

def get_pod_metrics_files_mapping(date: str) -> Dict[str, str]:
    """
    è·å– Pod æŒ‡æ ‡æ–‡ä»¶åæ˜ å°„ï¼Œè¿”å›æŒ‡æ ‡åç§°åˆ°æ–‡ä»¶åçš„æ˜ å°„å…³ç³»

    å‚æ•°:
        date: æ—¥æœŸï¼Œæ ¼å¼å¦‚ "2025-06-06"

    è¿”å›:
        æŒ‡æ ‡ååˆ°æ–‡ä»¶åçš„æ˜ å°„å­—å…¸
    """
    return {
        'pod_cpu_usage': f'infra_pod_pod_cpu_usage_{date}.parquet',
        'pod_fs_reads_bytes': f'infra_pod_pod_fs_reads_bytes_{date}.parquet',
        'pod_fs_writes_bytes': f'infra_pod_pod_fs_writes_bytes_{date}.parquet',
        'pod_memory_working_set_bytes': f'infra_pod_pod_memory_working_set_bytes_{date}.parquet',
        'pod_network_receive_bytes': f'infra_pod_pod_network_receive_bytes_{date}.parquet',
        'pod_network_receive_packets': f'infra_pod_pod_network_receive_packets_{date}.parquet',
        'pod_network_transmit_bytes': f'infra_pod_pod_network_transmit_bytes_{date}.parquet',
        'pod_network_transmit_packets': f'infra_pod_pod_network_transmit_packets_{date}.parquet',
        'pod_processes': f'infra_pod_pod_processes_{date}.parquet'
    }


# ==================== 2. ç›®æ ‡ Pod åˆ—è¡¨ ====================

def get_target_pods() -> List[str]:
    """
    è·å–ç›®æ ‡åˆ†æ Pod åˆ—è¡¨
    """
    services = [
        "adservice-0", "adservice-1", "adservice-2",
        "cartservice-0", "cartservice-1", "cartservice-2",
        "checkoutservice-0", "checkoutservice-1", "checkoutservice-2",
        "currencyservice-0", "currencyservice-1", "currencyservice-2",
        "emailservice-0", "emailservice-1", "emailservice-2",
        "frontend-0", "frontend-1", "frontend-2",
        "paymentservice-0", "paymentservice-1", "paymentservice-2",
        "productcatalogservice-0", "productcatalogservice-1", "productcatalogservice-2",
        "recommendationservice-0", "recommendationservice-1", "recommendationservice-2",
        "redis-cart-0",
        "shippingservice-0", "shippingservice-1", "shippingservice-2"
    ]
    return services


# ==================== 3. åŠ è½½ Pod æŒ‡æ ‡æ•°æ® ====================

def load_pod_metric_data(date: str, metric_name: str) -> Optional[pd.DataFrame]:
    """
    åŠ è½½æŒ‡å®šæ—¥æœŸå’ŒæŒ‡æ ‡çš„ Pod æ•°æ®

    å‚æ•°:
        date: æ—¥æœŸï¼Œæ ¼å¼å¦‚ "2025-06-06"
        metric_name: æŒ‡æ ‡åç§°ï¼Œå¦‚ "pod_cpu_usage"

    è¿”å›:
        Pod æŒ‡æ ‡æ•°æ® DataFrameï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å› None
    """
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    pod_dir = os.path.join(project_root, 'data', 'processed', f'{date}', 'metric-parquet', 'infra', 'infra_pod')

    file_mapping = get_pod_metrics_files_mapping(date)

    if metric_name not in file_mapping:
        print(f"æ•…éšœçš„æŒ‡æ ‡åç§°: {metric_name}")
        return None

    file_path = os.path.join(pod_dir, file_mapping[metric_name])

    try:
        if not os.path.exists(file_path):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None

        df = pd.read_parquet(file_path)

        # åªä¿ç•™ç›®æ ‡ pod æ•°æ®
        target_pods = get_target_pods()
        df_filtered = df[df['pod'].isin(target_pods)]

        if len(df_filtered) == 0:
            print(f"æ–‡ä»¶ {file_path} ä¸­æœªæ‰¾åˆ°ç›®æ ‡ pod æ•°æ®")
            return None

        return df_filtered

    except Exception as e:
        print(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        return None


# ==================== 4. æ—¶é—´è¿‡æ»¤ç»Ÿè®¡ ====================

def get_pod_metrics_description_with_time_filter(df_pod: pd.DataFrame, start_time: str, end_time: str,
                                                 metric_column: str, remove_outliers: bool = False) -> Optional[
    pd.Series]:
    """
    è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†… Pod æŒ‡æ ‡çš„æè¿°ç»Ÿè®¡
    """
    if 'timestamp_ns' not in df_pod.columns:
        print("è­¦å‘Š: æœªæ‰¾åˆ° timestamp_ns åˆ—ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´è¿‡æ»¤")
        return None

    # æ—¶é—´è¿‡æ»¤
    start_ts = int(start_time)
    end_ts = int(end_time)
    df_filtered = df_pod[(df_pod['timestamp_ns'] >= start_ts) & (df_pod['timestamp_ns'] <= end_ts)]

    if len(df_filtered) == 0:
        print("æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ— æ•°æ®")
        return None

    # è·å–æŒ‡æ ‡æ•°æ®
    if metric_column not in df_filtered.columns:
        print(f"è­¦å‘Š: åˆ— '{metric_column}' ä¸å­˜åœ¨äº DataFrame ä¸­")
        return None

    metric_data = df_filtered[metric_column].dropna()

    if len(metric_data) == 0:
        print(f"æŒ‡æ ‡ '{metric_column}' æ— æœ‰æ•ˆæ•°æ®")
        return None

    # æ˜¯å¦ç§»é™¤å¼‚å¸¸å€¼
    if remove_outliers and len(metric_data) > 4:
        metric_data_sorted = metric_data.sort_values()
        metric_data = metric_data_sorted.iloc[2:-2]  # å»æ‰æœ€å°2ä¸ªå’Œæœ€å¤§2ä¸ª
        print(f"ç§»é™¤å¼‚å¸¸å€¼åæ•°æ®ç‚¹æ•°é‡: {len(metric_data)}")
    # ç”Ÿæˆæè¿°ç»Ÿè®¡
    desc = metric_data.describe(percentiles=[0.25, 0.5, 0.75, 0.95, 0.99])

    # **æ–°å¢éé›¶æ¯”ä¾‹**
    desc['non_zero_ratio'] = round((metric_data != 0).sum() / len(metric_data), 3)

    return desc


# ==================== 5. æŒ‰ Pod åˆ†ææ•…éšœ vs æ­£å¸¸ ====================

def analyze_pod_metrics_by_pod(df_fault_timestamps: pd.DataFrame, index: int,
                               target_metrics: List[str] = None) -> Optional[Dict]:
    """
    åˆ†ææŒ‡å®šæ•…éšœæ—¶é—´æ®µä¸æ­£å¸¸æ—¶é—´æ®µçš„ Pod æŒ‡æ ‡å¯¹æ¯”
    ç»“æ„ï¼špod â†’ metric â†’ {normal_periods_combined, fault_period}
    """
    if target_metrics is None:
        target_metrics = [
            'pod_cpu_usage', 'pod_fs_reads_bytes', 'pod_fs_writes_bytes',
            'pod_memory_working_set_bytes', 'pod_network_receive_bytes',
            'pod_network_receive_packets', 'pod_network_transmit_bytes',
            'pod_network_transmit_packets', 'pod_processes'
        ]

    # è·å–æ•…éšœæ—¶é—´ä¿¡æ¯
    _, date, fault_start, fault_end = get_fault_period_info(df_fault_timestamps, index)
    normal_periods = get_normal_time_periods(df_fault_timestamps, index)
    target_pods = get_target_pods()

    print(f"Pod åˆ†æ - æ•…éšœæ—¥æœŸ: {date}")
    print(f"Pod åˆ†æ - æ•…éšœæ—¶é—´æ®µ: {fault_start} è‡³ {fault_end}")
    print(f"Pod åˆ†æ - æ­£å¸¸æ—¶é—´æ®µæ•°é‡: {len(normal_periods)}")
    print(f"Pod åˆ†æ - ç›®æ ‡ Pod æ•°é‡: {len(target_pods)}")
    print(f"Pod åˆ†æ - åˆ†ææŒ‡æ ‡æ•°é‡: {len(target_metrics)}")

    # æŒ‰ Pod â†’ æŒ‡æ ‡ â†’ æ—¶é—´æ®µ ç»“æ„ç»„ç»‡åˆ†æç»“æœ
    pods_analysis = {}

    for pod_name in target_pods:
        print(f"\n=== å¤„ç† Pod: {pod_name} ===")

        pods_analysis[pod_name] = {}

        for metric_name in target_metrics:
            print(f"  å¤„ç†æŒ‡æ ‡: {metric_name}")

            # åŠ è½½è¯¥æŒ‡æ ‡çš„æ•°æ®
            df_metric = load_pod_metric_data(date, metric_name)

            if df_metric is None:
                print(f"    æ— æ³•åŠ è½½æŒ‡æ ‡ {metric_name} çš„æ•°æ®ï¼Œè·³è¿‡")
                continue

            # è¿‡æ»¤å½“å‰ Pod çš„æ•°æ®
            df_pod = df_metric[df_metric['pod'] == pod_name]
            # åˆ é™¤ device åˆ—ä¸º /dev/vdb çš„è¡Œ
            if 'device' in df_pod.columns:
                df_pod = df_pod[df_pod['device'] != '/dev/dmb']

            if len(df_pod) == 0:
                print(f"    Pod {pod_name} æ—  {metric_name} æ•°æ®")
                continue

            # åˆå§‹åŒ–æŒ‡æ ‡ç»“æ„
            pods_analysis[pod_name][metric_name] = {
                'normal_periods_combined': None,
                'fault_period': None
            }

            # 1. åˆå¹¶æ‰€æœ‰æ­£å¸¸æ—¶é—´æ®µæ•°æ®
            print(f"    æ­£å¸¸æ—¶é—´æ®µåˆ†æ:")
            all_normal_data = []

            for i, (normal_start, normal_end) in enumerate(normal_periods):
                start_ts = int(normal_start)
                end_ts = int(normal_end)
                normal_data = df_pod[(df_pod['timestamp_ns'] >= start_ts) & (df_pod['timestamp_ns'] <= end_ts)]

                if len(normal_data) > 0:
                    all_normal_data.append(normal_data)
                    print(f"      æ—¶é—´æ®µ {i + 1} æ•°æ®è¡Œæ•°: {len(normal_data)}")

            # åˆå¹¶æ­£å¸¸æ—¶é—´æ®µæ•°æ®å¹¶ç»Ÿè®¡
            if all_normal_data:
                combined_normal_data = pd.concat(all_normal_data, ignore_index=True)
                print(f"    åˆå¹¶åæ­£å¸¸æ—¶é—´æ®µæ€»æ•°æ®è¡Œæ•°: {len(combined_normal_data)}")

                normal_desc = get_pod_metrics_description_with_time_filter(
                    combined_normal_data,
                    str(combined_normal_data['timestamp_ns'].min()),
                    str(combined_normal_data['timestamp_ns'].max()),
                    metric_name,
                    remove_outliers=(len(combined_normal_data) > 4)
                )
                if normal_desc is not None:
                    print(f"    æ­£å¸¸æœŸé—´ {metric_name}: mean={normal_desc['mean']:.2f}, std={normal_desc['std']:.2f}")

            # 2. æ•…éšœæ—¶é—´æ®µç»Ÿè®¡
            print(f"    æ•…éšœæ—¶é—´æ®µåˆ†æ:")
            fault_desc = get_pod_metrics_description_with_time_filter(
                df_pod, fault_start, fault_end, metric_name, remove_outliers=False
            )
            # if normal_desc is not None and fault_desc is not None:#è¿‡æ»¤æ‰å˜åŒ–å€æ•°åœ¨ 0.95 åˆ° 1.05 ä¹‹é—´çš„æŒ‡æ ‡
            #     normal_mean = normal_desc['mean']
            #     fault_mean = fault_desc['mean']
            #     epsilon = 1e-9  # æå°æ•°ï¼Œé˜²æ­¢é™¤é›¶
            #     ratio = (fault_mean + epsilon) / (normal_mean + epsilon)
            #
            #     if 0.95 <= ratio <= 1.05:
            #         print(f"    æŒ‡æ ‡ {metric_name} å˜åŒ–å€æ•° {ratio:.2f} åœ¨ 0.95~1.05 ä¹‹é—´ï¼Œè·³è¿‡ä¿å­˜")
            #         continue
            pods_analysis[pod_name][metric_name]['fault_period'] = fault_desc
            pods_analysis[pod_name][metric_name]['normal_periods_combined'] = normal_desc
            if fault_desc is not None:
                fault_data_count = len(df_pod[(df_pod['timestamp_ns'] >= int(fault_start)) &
                                              (df_pod['timestamp_ns'] <= int(fault_end))])
                print(f"    æ•…éšœæ—¶é—´æ®µæ•°æ®è¡Œæ•°: {fault_data_count}")
                print(f"    æ•…éšœæœŸé—´ {metric_name}: mean={fault_desc['mean']:.2f}, std={fault_desc['std']:.2f}")

    return pods_analysis if pods_analysis else None


def call_llm_analysis(prompt: str, uuid: str = None, call_type: str = "æœªçŸ¥ç±»å‹") -> str:
    """
    è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œåˆ†æ

    å‚æ•°:
        prompt: åˆ†æprompt
        uuid: æ ·æœ¬UUIDï¼Œç”¨äºè®°å½•
        call_type: è°ƒç”¨ç±»å‹ï¼Œç”¨äºè®°å½•

    è¿”å›:
        å¤§æ¨¡å‹çš„åˆ†æç»“æœ
    """
    try:
        llm_agent = create_llm()
        messages = [
            {
                "content": prompt,
                "role": "user",
            }
        ]
        reply = llm_agent.generate_reply(messages)

        response_content = reply['content'] if isinstance(reply, dict) and 'content' in reply else reply
        # è®°å½•å¤§æ¨¡å‹è°ƒç”¨
        if uuid:
            try:
                from utils.llm_record_utils import record_llm_call
                record_llm_call(uuid, call_type, prompt, response_content)
            except Exception as record_e:
                print(f"è®°å½•å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {record_e}")

        return response_content
    except Exception as e:
        print(f"è°ƒç”¨å¤§æ¨¡å‹æ—¶å‡ºé”™: {e}")
        error_msg = f"å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}"

        # å³ä½¿å‡ºé”™ä¹Ÿè®°å½•
        if uuid:
            try:
                from utils.llm_record_utils import record_llm_call
                record_llm_call(uuid, f"{call_type}(å¤±è´¥)", prompt, error_msg)
            except Exception as record_e:
                print(f"è®°å½•å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {record_e}")

        return error_msg


def get_node_pod_mapping(date: str) -> Dict[str, List[str]]:
    """
    è·å–æ¯ä¸ªèŠ‚ç‚¹ä¸Šéƒ¨ç½²çš„Podåˆ—è¡¨

    å‚æ•°:
        date: æ—¥æœŸï¼Œæ ¼å¼å¦‚ "2025-06-06"

    è¿”å›:
        èŠ‚ç‚¹åˆ°Podåˆ—è¡¨çš„æ˜ å°„å­—å…¸ {node_name: [pod1, pod2, ...]}
    """
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    infra_pod_dir = os.path.join(project_root, 'data', 'processed', f'{date}', 'metric-parquet', 'infra', 'infra_pod')

    # ä¼˜å…ˆå°è¯•è¯»å–CPUä½¿ç”¨ç‡æ–‡ä»¶
    target_file = f'infra_pod_pod_cpu_usage_{date}.parquet'
    target_file_path = os.path.join(infra_pod_dir, target_file)

    df_pod_info = None

    try:
        if os.path.exists(target_file_path):
            print(f"ä½¿ç”¨ç›®æ ‡æ–‡ä»¶è·å–Podéƒ¨ç½²ä¿¡æ¯: {target_file}")
            df_pod_info = pd.read_parquet(target_file_path)
        else:
            # å¦‚æœç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªæ–‡ä»¶
            if os.path.exists(infra_pod_dir):
                available_files = [f for f in os.listdir(infra_pod_dir) if f.endswith('.parquet')]
                if available_files:
                    selected_file = available_files[0]  # é€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡ä»¶
                    selected_file_path = os.path.join(infra_pod_dir, selected_file)
                    print(f"ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å¤‡é€‰æ–‡ä»¶è·å–Podéƒ¨ç½²ä¿¡æ¯: {selected_file}")
                    df_pod_info = pd.read_parquet(selected_file_path)
                else:
                    print("infra_podç›®å½•ä¸­æ²¡æœ‰å¯ç”¨çš„parquetæ–‡ä»¶")
                    return {}
            else:
                print(f"infra_podç›®å½•ä¸å­˜åœ¨: {infra_pod_dir}")
                return {}

        if df_pod_info is None or len(df_pod_info) == 0:
            print("æ— æ³•è¯»å–Podéƒ¨ç½²ä¿¡æ¯")
            return {}

        # è·å–ç›®æ ‡èŠ‚ç‚¹åˆ—è¡¨
        target_nodes = get_target_nodes()
        node_pod_mapping = {}

        for node_name in target_nodes:
            # ç­›é€‰è¯¥èŠ‚ç‚¹çš„æ•°æ®
            node_data = df_pod_info[df_pod_info['instance'] == node_name]
            if len(node_data) > 0:
                # è·å–è¯¥èŠ‚ç‚¹ä¸Šçš„å”¯ä¸€Podåˆ—è¡¨
                pods_on_node = node_data['pod'].unique().tolist()
                node_pod_mapping[node_name] = pods_on_node
                print(f"èŠ‚ç‚¹ {node_name} éƒ¨ç½²çš„Podæ•°é‡: {len(pods_on_node)}")
            else:
                print(f"èŠ‚ç‚¹ {node_name} æœªæ‰¾åˆ°Podéƒ¨ç½²ä¿¡æ¯")
                node_pod_mapping[node_name] = []

        return node_pod_mapping

    except Exception as e:
        print(f"è·å–Podéƒ¨ç½²ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return {}


def create_combined_node_prompt_with_service_analysis(node_analysis_results: Dict, pod_result: Dict,
                                                      service_analysis_result: str,
                                                      node_pod_mapping: Dict[str, List[str]]) -> str:
    """
    åˆå¹¶æ‰€æœ‰èŠ‚ç‚¹çš„promptä¸ºä¸€ä¸ªç»¼åˆpromptï¼ŒåŒ…å«serviceåˆ†æç»“æœå’Œpodéƒ¨ç½²ä¿¡æ¯

    å‚æ•°:
        node_analysis_results: èŠ‚ç‚¹åˆ†æç»“æœ
        service_analysis_result: serviceçº§åˆ«çš„LLMåˆ†æç»“æœ
        node_pod_mapping: èŠ‚ç‚¹åˆ°Podåˆ—è¡¨çš„æ˜ å°„

    è¿”å›:
        åˆå¹¶åçš„ç»¼åˆprompt
    """
    if not node_analysis_results:
        return ""

    # è·å–æ‰€æœ‰èŠ‚ç‚¹ä¿¡æ¯
    all_nodes = list(node_analysis_results.keys())
    all_metrics = set()
    for node_data in node_analysis_results.values():
        all_metrics.update(node_data.keys())
    all_metrics = list(all_metrics)

    # æŒ‡æ ‡ä¸­æ–‡åç§°æ˜ å°„
    metric_chinese_names = {
        'node_cpu_usage_rate': 'CPUä½¿ç”¨ç‡',
        'node_disk_read_bytes_total': 'ç£ç›˜è¯»å–å­—èŠ‚æ•°',
        'node_disk_read_time_seconds_total': 'ç£ç›˜è¯»å–æ—¶é—´',
        'node_disk_write_time_seconds_total': 'ç£ç›˜å†™å…¥æ—¶é—´',
        'node_disk_written_bytes_total': 'ç£ç›˜å†™å…¥å­—èŠ‚æ•°',
        'node_filesystem_free_bytes': 'ç©ºé—²ç£ç›˜å¤§å°',
        'node_filesystem_size_bytes': 'ç£ç›˜æ€»å¤§å°',
        'node_filesystem_usage_rate': 'æ–‡ä»¶ç³»ç»Ÿä½¿ç”¨ç‡',
        'node_memory_MemAvailable_bytes': 'ç©ºé—²å†…å­˜å¤§å°',
        'node_memory_MemTotal_bytes': 'å†…å­˜æ€»å¤§å°',
        'node_memory_usage_rate': 'å†…å­˜ä½¿ç”¨ç‡',
        'node_network_receive_bytes_total': 'ç½‘ç»œæ¥æ”¶å­—èŠ‚æ•°',
        'node_network_receive_packets_total': 'Receive å„ä¸ªæ¥å£æ¯ç§’æ¥æ”¶çš„æ•°æ®åŒ…æ€»æ•°',
        'node_network_transmit_bytes_total': 'Transmit å„ä¸ªç½‘ç»œæ¥å£å‘é€é€Ÿç‡',
        'node_network_transmit_packets_total': 'Transmit å„ä¸ªæ¥å£æ¯ç§’å‘é€çš„æ•°æ®åŒ…æ€»æ•°',
        'node_sockstat_TCP_inuse': 'TCPè¿æ¥æ•°',
    }

    # æ„å»ºèŠ‚ç‚¹Podéƒ¨ç½²ä¿¡æ¯
    node_deployment_info = []
    total_pods = 0

    # for node_name in all_nodes:
    #     pods_on_node = node_pod_mapping.get(node_name, [])
    #     total_pods += len(pods_on_node)
    #     node_deployment_info.append(f"\n### èŠ‚ç‚¹: {node_name}")
    #     node_deployment_info.append(f"- **éƒ¨ç½²Podæ•°é‡**: {len(pods_on_node)}")
    #     if pods_on_node:
    #         node_deployment_info.append(f"- **Podåˆ—è¡¨**: {pods_on_node}")
    #     else:
    #         node_deployment_info.append(f"- **Podåˆ—è¡¨**: æ— æ•°æ®")

    # deployment_info_text = "\n".join(node_deployment_info)

    # æ„å»ºåˆå¹¶çš„æ•°æ®å¯¹æ¯”è¡¨æ ¼
    combined_json = {}

    # æå–å…¨å±€ä¸€è‡´çš„ metric_count å’Œ metric_list
    # å‡è®¾æ‰€æœ‰ node_data çš„æŒ‡æ ‡ä¸€è‡´ï¼Œç”¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„æ•°æ®å³å¯
    first_node_data = next(iter(node_analysis_results.values()))
    combined_json["metric_count"] = len(first_node_data)
    combined_json["metric_list"] = list(first_node_data.keys())

    # ç”¨ nodes å­˜å‚¨æ¯ä¸ªèŠ‚ç‚¹çš„ç‹¬ç«‹æ•°æ®
    combined_json["nodes"] = {}

    for node_name, node_data in node_analysis_results.items():
        pods_on_node = node_pod_mapping.get(node_name, [])

        # åˆå§‹åŒ–èŠ‚ç‚¹ä¿¡æ¯
        node_info = {
            "node_name": node_name,
            "pod_count": len(pods_on_node),
            "pods": pods_on_node,  # Podåç§°åˆ—è¡¨
            "metrics": {},  # èŠ‚ç‚¹æŒ‡æ ‡ä¿¡æ¯
            "pods_detail": {}  # è¯¥èŠ‚ç‚¹ä¸‹Podçš„è¯¦ç»†æŒ‡æ ‡
        }

        # ---- å¤„ç†èŠ‚ç‚¹æŒ‡æ ‡ ----
        for metric_name, metric_stats in node_data.items():
            normal_stats = metric_stats.get('normal_periods_combined')
            fault_stats = metric_stats.get('fault_period')

            if normal_stats is not None and fault_stats is not None:
                normal_mean = normal_stats.get('mean', 0)
                normal_std = normal_stats.get('std', 0)
                normal_max = normal_stats.get('max', 0)
                normal_p99 = normal_stats.get('99%', 0)
                normal_p50 = normal_stats.get('50%', 0)
                fault_mean = fault_stats.get('mean', 0)
                fault_std = fault_stats.get('std', 0)
                fault_max = fault_stats.get('max', 0)
                fault_nzr = fault_stats.get('non_zero_ratio',0)
                normal_nzr = normal_stats.get('non_zero_ratio', 0)
                fault_p99 = fault_stats.get('99%', 0)
                fault_p50 = fault_stats.get('50%', 0)
                normal_p75 = normal_stats.get('75%', 0)
                normal_p25 = normal_stats.get('25%', 0)
                normal_iqr = normal_p75 - normal_p25

                fault_p75 = fault_stats.get('75%', 0)
                fault_p25 = fault_stats.get('25%', 0)
                fault_iqr = fault_p75 - fault_p25
                # è®¡ç®—å¯¹ç§°æ¯”ç‡
                p50_symmetric_ratio = abs(fault_p50 - normal_p50) / (
                        (fault_p50 + normal_p50) / 2 + 1e-9
                )
                p99_symmetric_ratio = abs(fault_p99 - normal_p99) / (
                        (fault_p99 + normal_p99) / 2 + 1e-9
                )
                # è¿‡æ»¤é˜ˆå€¼ï¼ˆå»ºè®® 0.5 æˆ– 1ï¼Œæ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
                if p50_symmetric_ratio < 0.05 and p99_symmetric_ratio <0.05:
                    continue

                node_info["metrics"][metric_name] = {
                    "æ­£å¸¸æœŸé—´ä¸­ä½æ•°": round(normal_p50, 2),
                    "æ­£å¸¸æœŸé—´å››åˆ†ä½è·": round(normal_iqr, 2),
                    "æ­£å¸¸æœŸé—´99åˆ†ä½æ•°": round(normal_p99, 2),
                    "æ•…éšœæœŸé—´ä¸­ä½æ•°": round(fault_p50, 2),
                    "æ•…éšœæœŸé—´å››åˆ†ä½è·": round(fault_iqr, 2),
                    "æ•…éšœæœŸé—´99åˆ†ä½æ•°": round(fault_p99, 2)
                }
            else:
                node_info["metrics"][metric_name] = "ç¼ºå¤±æ•°æ®"

        # ---- å¤„ç†èŠ‚ç‚¹ä¸‹çš„ Pods æŒ‡æ ‡ ----
        for pod_name in pods_on_node:
            pod_metrics = pod_result.get(pod_name, {})  # è·å–å½“å‰ Pod çš„æŒ‡æ ‡æ•°æ®

            pod_detail = {}
            for metric_name, metric_stats in pod_metrics.items():
                normal_stats = metric_stats.get('normal_periods_combined')
                fault_stats = metric_stats.get('fault_period')

                if normal_stats is not None and fault_stats is not None:
                    normal_mean = normal_stats.get('mean', 0)
                    normal_std = normal_stats.get('std', 0)
                    normal_max = normal_stats.get('max', 0)
                    normal_p99 = normal_stats.get('99%', 0)
                    normal_p50 = normal_stats.get('50%', 0)
                    normal_p25 = normal_stats.get('25%', 0)
                    normal_p75 = normal_stats.get('75%', 0)
                    fault_mean = fault_stats.get('mean', 0)
                    fault_std = fault_stats.get('std', 0)
                    fault_max = fault_stats.get('max', 0)
                    fault_p99 = fault_stats.get('99%', 0)
                    fault_p50 = fault_stats.get('50%', 0)
                    fault_p25 = fault_stats.get('25%', 0)
                    fault_p75 = fault_stats.get('75%', 0)
                    normal_iqr = normal_p75 - normal_p25
                    fault_iqr = fault_p75 - fault_p25
                    fault_nzr = fault_stats.get('non_zero_ratio', 0)
                    normal_nzr = normal_stats.get('non_zero_ratio', 0)
                    # è®¡ç®—å¯¹ç§°æ¯”ç‡
                    p50_symmetric_ratio = abs(fault_p50 - normal_p50) / (
                            (fault_p50 + normal_p50) / 2 + 1e-9
                    )
                    p99_symmetric_ratio = abs(fault_p99 - normal_p99) / (
                            (fault_p99 + normal_p99) / 2 + 1e-9
                    )
                    # è¿‡æ»¤é˜ˆå€¼ï¼ˆå»ºè®® 0.5 æˆ– 1ï¼Œæ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
                    if p50_symmetric_ratio < 0.05 and p99_symmetric_ratio < 0.05:
                        continue

                    pod_detail[metric_name] = {
                        "æ­£å¸¸æœŸé—´ä¸­ä½æ•°": round(normal_p50, 2),
                        "æ­£å¸¸æœŸé—´å››åˆ†ä½è·": round(normal_iqr, 2),
                        "æ­£å¸¸æœŸé—´99åˆ†ä½æ•°": round(normal_p99, 2),
                        "æ•…éšœæœŸé—´ä¸­ä½æ•°": round(fault_p50, 2),
                        "æ•…éšœæœŸé—´å››åˆ†ä½è·": round(fault_iqr, 2),
                        "æ•…éšœæœŸé—´99åˆ†ä½æ•°": round(fault_p99, 2)
                    }
                else:
                    pod_detail[metric_name] = "ç¼ºå¤±æ•°æ®"

            # å°†è¯¥ Pod è¯¦ç»†æŒ‡æ ‡å†™å…¥èŠ‚ç‚¹ä¸‹
            node_info["pods_detail"][pod_name] = pod_detail

        # å°†èŠ‚ç‚¹ä¿¡æ¯æ”¾å…¥æ€» JSON
        combined_json["nodes"][node_name] = node_info
        # combined_json= compress_combined_json(combined_json)
    return f"""
è¯·åŸºäºæä¾›çš„Serviceçº§åˆ«åˆ†æç»“æœã€Podéƒ¨ç½²ä¿¡æ¯å’ŒåŸºç¡€è®¾æ–½ç›‘æ§æŒ‡æ ‡æ•°æ®ï¼Œè¿›è¡Œå…¨å±€çš„ç°è±¡æ€»ç»“åˆ†æã€‚

## Serviceçº§åˆ«åˆ†æç»“æœå›é¡¾
{service_analysis_result}

## é›†ç¾¤åŸºç¡€è®¾æ–½ä¿¡æ¯
- **èŠ‚ç‚¹æ€»æ•°**: {len(all_nodes)}
- **ç›‘æ§æŒ‡æ ‡æ€»æ•°**: {len(all_metrics)}
- **é›†ç¾¤æ€»Podæ•°**: {total_pods}
- **èŠ‚ç‚¹åˆ—è¡¨**: {all_nodes}
- **ç›‘æ§æŒ‡æ ‡**: {[metric_chinese_names.get(m, m) for m in all_metrics]}

## ä½¿ç”¨è§„èŒƒè¯´æ˜
â€¢ æ‰€æœ‰ç›‘æ§æŒ‡æ ‡å‡ä¸º `kpi_key` æŒ‡æ ‡ï¼ˆå¦‚ `node_cpu_usage_rate`ï¼‰ï¼Œè¯·å§‹ç»ˆä½¿ç”¨è¿™äº›åŸå§‹è‹±æ–‡åç§°è¿›è¡Œåˆ†æä¸è¾“å‡ºï¼›
â€¢ ä¸¥ç¦ä½¿ç”¨ä¸­æ–‡æˆ–ç¼©å†™å½¢å¼ï¼ˆå¦‚"CPU"ã€"CPUä½¿ç”¨ç‡"ã€"ç£ç›˜è¯»å†™"ç­‰ï¼‰ä»£æ›¿ï¼›
â€¢ å¿…é¡»æ˜¾å¼åŒ…å«å¯¹åº”çš„ `kpi_key`ï¼Œ æ¯æ¬¡æåŠ`kpi_key`æŒ‡æ ‡å¦‚( `node_cpu_usage_rate`ï¼‰æ—¶ï¼Œå¿…é¡»æ˜¾å¼åŒ…å«å¯¹åº”çš„ `kpi_key`ï¼Œå¿…é¡»æ˜¾å¼æŒ‡å‡ºè¿™æ˜¯ä¸€ä¸ª `kpi_key` æŒ‡æ ‡å¦‚ï¼š
  â€¢ é”™è¯¯ç¤ºä¾‹ï¼šèŠ‚ç‚¹çº§æŒ‡æ ‡å˜åŒ–å¹…åº¦ï¼ˆCPU â†‘23%ï¼‰
  â€¢ æ­£ç¡®ç¤ºä¾‹ï¼škpi_keyæŒ‡æ ‡`node_cpu_usage_rate` åœ¨èŠ‚ç‚¹ aiops-k8s-08 ä¸Šä¸Šå‡äº† 23%

## åŸºç¡€è®¾æ–½æŒ‡æ ‡åˆ†ç±»è¯´æ˜

### è®¡ç®—èµ„æºç±»æŒ‡æ ‡(kpi_key)ï¼š
- **node_cpu_usage_rate**: CPUä½¿ç”¨ç‡, åæ˜ èŠ‚ç‚¹CPUä½¿ç”¨ç‡  
- **node_memory_usage_rate**: å†…å­˜ä½¿ç”¨ç‡, åæ˜ èŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡  
- **pod_cpu_usage**: Pod CPUä½¿ç”¨ç‡  
- **pod_memory_working_set_bytes**: Podå·¥ä½œé›†å†…å­˜ä½¿ç”¨é‡  
- **pod_processes**: Podå†…è¿è¡Œè¿›ç¨‹æ•°é‡  

### å­˜å‚¨èµ„æºç±»æŒ‡æ ‡(kpi_key)ï¼š
- **node_filesystem_usage_rate**: æ–‡ä»¶ç³»ç»Ÿä½¿ç”¨ç‡, åæ˜ èŠ‚ç‚¹å­˜å‚¨ç©ºé—´ä½¿ç”¨ç‡  
- **node_disk_read_bytes_total / node_disk_read_time_seconds_total**: ç£ç›˜è¯»å–å­—èŠ‚æ•°/æ—¶é—´, åæ˜ ç£ç›˜è¯»å–æ€§èƒ½  
- **node_disk_written_bytes_total / node_disk_write_time_seconds_total**: ç£ç›˜å†™å…¥å­—èŠ‚æ•°/æ—¶é—´, åæ˜ ç£ç›˜å†™å…¥æ€§èƒ½  
- **pod_fs_reads_bytes**: Pod æ–‡ä»¶ç³»ç»Ÿè¯»å–å­—èŠ‚æ•°  
- **pod_fs_writes_bytes**: Pod æ–‡ä»¶ç³»ç»Ÿå†™å…¥å­—èŠ‚æ•°  

### ç½‘ç»œèµ„æºç±»æŒ‡æ ‡(kpi_key)ï¼š
- **node_network_receive_bytes_total**: ç½‘ç»œæ¥æ”¶å­—èŠ‚æ•°, åæ˜ èŠ‚ç‚¹ç½‘ç»œæ¥æ”¶æµé‡  
- **node_network_transmit_bytes_total**: ç½‘ç»œå‘é€å­—èŠ‚æ•°, åæ˜ èŠ‚ç‚¹ç½‘ç»œå‘é€æµé‡  
- **node_network_receive_packets_total**: å„æ¥å£æ¯ç§’æ¥æ”¶çš„æ•°æ®åŒ…æ€»æ•°  
- **node_network_transmit_packets_total**: å„æ¥å£æ¯ç§’å‘é€çš„æ•°æ®åŒ…æ€»æ•°  
- **node_sockstat_TCP_inuse**: TCPè¿æ¥æ•°, åæ˜ èŠ‚ç‚¹TCPè¿æ¥æ´»è·ƒåº¦  
- **pod_network_receive_bytes**: Pod ç½‘ç»œæ¥æ”¶å­—èŠ‚æ•°  
- **pod_network_receive_packets**: Pod ç½‘ç»œæ¥æ”¶æ•°æ®åŒ…æ•°  
- **pod_network_transmit_bytes**: Pod ç½‘ç»œå‘é€å­—èŠ‚æ•°  
- **pod_network_transmit_packets**: Pod ç½‘ç»œå‘é€æ•°æ®åŒ…æ•°  

## åŸºç¡€è®¾æ–½æŒ‡æ ‡æ•°æ®å¯¹æ¯”è¡¨æ ¼,ç‰¹åˆ«æ³¨æ„**ç¼ºå¤±æ•°æ®å’Œç©ºæ•°æ®,ä»£è¡¨æ•°æ®æ³¢åŠ¨æå°,é€šå¸¸æƒ…å†µé»˜è®¤æ­£å¸¸**:
{json.dumps(combined_json, ensure_ascii=False, indent=2)}

## ç»¼åˆç°è±¡åˆ†æè¦æ±‚
è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œå…¨å±€ç°è±¡æè¿°ï¼Œ**ä»…æè¿°è§‚å¯Ÿåˆ°çš„ç°è±¡ï¼Œä¸åšå¼‚å¸¸åˆ¤æ–­æˆ–ç»“è®º**ï¼š

### 1. Nodeçº§åˆ«ç°è±¡è§‚å¯Ÿ
åŸºäºNodeçš„æ­£å¸¸æ—¶é—´æ®µä¸å¼‚å¸¸æ—¶é—´æ®µçš„æ•°æ®å¯¹æ¯”ï¼Œæè¿°ï¼š
- åŒä¸€Nodeçš„æ­£å¸¸æ—¶é—´æ®µå’Œå¼‚å¸¸æ—¶é—´æ®µï¼Œæ˜¾è‘—çš„æŒ‡æ ‡å¼‚å¸¸å˜åŒ–æˆ–æ˜¾è‘—çš„ `kpi_key` æŒ‡æ ‡å¼‚å¸¸å˜åŒ–
- ä¸åŒNodeä¹‹é—´æ¯”è¾ƒè¡¨ç°å‡ºçš„å¼‚å¸¸å·®å¼‚

### 2. Serviceçº§åˆ«ç°è±¡è§‚å¯Ÿ
- é›†ä¸­åœ¨åŒä¸€ç±»å‹çš„æœåŠ¡ä¸­å‡ºç°çš„é—®é¢˜ï¼Œæ¯”å¦‚emailservice-0, emailservice-1, emailservice-2, éƒ½å­˜åœ¨ç›¸ä¼¼çš„å¼‚å¸¸æ•°æ®å˜åŒ–ï¼Œåˆ™æè¿°å…·ä½“å˜åŒ–ç°è±¡ï¼Œå› ä¸ºè¿™å¯èƒ½æ˜¯emailserviceå­˜åœ¨æ½œåœ¨é—®é¢˜

### 3. Podçº§åˆ«ç°è±¡è§‚å¯Ÿ
- ä¸ªåˆ«Podçš„å¼‚å¸¸è¡¨ç°ç‰¹å¾
- å¦‚ cartservice-0 ä¸­å­˜åœ¨å¼‚å¸¸çš„æ•°æ®å˜åŒ–ï¼Œè€Œcartservice-1, cartservice-2ä»¥åŠå…¶ä»–podæ­£å¸¸ï¼Œåˆ™å¯èƒ½æ˜¯å•ç‹¬çš„podçº§åˆ«çš„å¼‚å¸¸ç°è±¡
- å¤§å¤šæ•°å¼‚å¸¸çš„Podæ˜¯å¦éƒ¨ç½²åœ¨åŒä¸€ä¸ªNodeï¼Œè¿˜æ˜¯åˆ†æ•£åœ¨ä¸åŒçš„Nodeï¼Œæ˜¯å¦å­˜åœ¨Nodeçº§åˆ«çš„å¼‚å¸¸ç°è±¡

## é‡è¦æç¤º
**è¿™æ˜¯ä¸ºåæœŸå¤šæ¨¡æ€ç»¼åˆå†³ç­–åˆ†ææä¾›çš„å…¨å±€ç°è±¡æ€»ç»“ï¼Œè¯·æ§åˆ¶æ€»ç»“å†…å®¹åœ¨2000å­—å·¦å³ï¼Œé‡ç‚¹çªå‡ºä¸»è¦å˜åŒ–ç°è±¡ã€‚**

**æ€»ç»“è¦æ±‚ï¼š**
- å¿…é¡»åœ¨è¾“å‡ºä¸­åŒ…å«åŸå§‹ `kpi_key` æŒ‡æ ‡åç§°ï¼Œå¦‚ `node_cpu_usage_rate`
- åˆ†æåŸå› æ—¶å¿…é¡»æ˜ç¡®æŒ‡å‡ºè¯¥åŸå› å±äºå“ªä¸ª `metric`ï¼ˆå…¶ä»–æŒ‡æ ‡ "ä»¥åŠ/æˆ–è€…" å“ªä¸ª`kpi_key`ï¼‰ä¸‹çš„è§‚å¯Ÿ
- å¦‚æœå‡ºç°æ˜æ˜¾å˜åŒ–ï¼Œè¯·é‡ç‚¹æè¿°å˜åŒ–æ˜¾è‘—çš„æœåŠ¡ã€æŒ‡æ ‡å’Œç°è±¡
- é‡‡ç”¨æ¦‚æ‹¬æ€§è¯­è¨€ï¼Œé‡ç‚¹å…³æ³¨å¯¹ä¸šåŠ¡å½±å“è¾ƒå¤§çš„æŒ‡æ ‡å˜åŒ–
- å…³æ³¨å¼‚å¸¸ç°è±¡å€¾å‘äºnodeï¼Œserviceè¿˜æ˜¯podçº§åˆ«çš„å¼‚å¸¸
- æä¾›ç³»ç»Ÿçº§çš„ç»¼åˆç°è±¡æè¿°ï¼Œä¸ºåç»­å†³ç­–æä¾›å…¨é¢è§†è§’
- é‡‡ç”¨å®¢è§‚ã€æè¿°æ€§çš„è¯­è¨€ï¼Œé¿å…ä¸»è§‚åˆ¤æ–­
- ç¼ºå¤±æˆ–ä¸ºç©ºçš„æ•°æ®è¡¨ç¤ºæ³¢åŠ¨æå°ï¼Œå¯è§†ä¸ºæ­£å¸¸ï¼Œæ— éœ€æè¿°

è¯·åŸºäºServiceåˆ†æã€Podéƒ¨ç½²ä¿¡æ¯å’ŒåŸºç¡€è®¾æ–½ç›‘æ§æ•°æ®ï¼Œæä¾›å…¨å±€çš„ç»¼åˆç°è±¡æ€»ç»“ï¼Œæ§åˆ¶åœ¨2000å­—ä»¥å†…ã€‚
"""


# ==================== TiDB æœåŠ¡ç›¸å…³å‡½æ•° ====================

def get_tidb_services_files_mapping(date: str) -> Dict[str, Dict[str, str]]:
    """
    è·å–TiDBæœåŠ¡çš„æ–‡ä»¶åæ˜ å°„ï¼Œè¿”å›æœåŠ¡ååˆ°æŒ‡æ ‡æ–‡ä»¶çš„æ˜ å°„å…³ç³»

    å‚æ•°:
        date: æ—¥æœŸï¼Œæ ¼å¼å¦‚ "2025-06-06"

    è¿”å›:
        æœåŠ¡ååˆ°æŒ‡æ ‡æ–‡ä»¶æ˜ å°„çš„å­—å…¸ {service_name: {metric_name: file_name}}
    """
    return {
        'tidb-tidb': {
            'failed_query_ops': f'infra_tidb_failed_query_ops_{date}.parquet',
            'duration_99th': f'infra_tidb_duration_99th_{date}.parquet',
            'connection_count': f'infra_tidb_connection_count_{date}.parquet',
            'server_is_up': f'infra_tidb_server_is_up_{date}.parquet',
            'cpu_usage': f'infra_tidb_cpu_usage_{date}.parquet',
            'memory_usage': f'infra_tidb_memory_usage_{date}.parquet'
        },
        'tidb-pd': {
            'store_up_count': f'infra_pd_store_up_count_{date}.parquet',
            'store_down_count': f'infra_pd_store_down_count_{date}.parquet',
            'cpu_usage': f'infra_pd_cpu_usage_{date}.parquet',
            'memory_usage': f'infra_pd_memory_usage_{date}.parquet',
            'storage_used_ratio': f'infra_pd_storage_used_ratio_{date}.parquet',
            'store_unhealth_count': f'infra_pd_store_unhealth_count_{date}.parquet'
        },
        'tidb-tikv': {
            'cpu_usage': f'infra_tikv_cpu_usage_{date}.parquet',
            'memory_usage': f'infra_tikv_memory_usage_{date}.parquet',
            'server_is_up': f'infra_tikv_server_is_up_{date}.parquet',
            'available_size': f'infra_tikv_available_size_{date}.parquet',
            'raft_propose_wait': f'infra_tikv_raft_propose_wait_{date}.parquet',
            'raft_apply_wait': f'infra_tikv_raft_apply_wait_{date}.parquet',
            'rocksdb_write_stall': f'infra_tikv_rocksdb_write_stall_{date}.parquet'
        }
    }


def get_tidb_services_directories() -> Dict[str, str]:
    """
    è·å–TiDBæœåŠ¡çš„æ•°æ®ç›®å½•æ˜ å°„

    è¿”å›:
        æœåŠ¡ååˆ°ç›®å½•è·¯å¾„çš„æ˜ å°„å­—å…¸
    """
    return {
        'tidb-tidb': 'infra/infra_tidb',
        'tidb-pd': 'other',
        'tidb-tikv': 'other'
    }


def get_tidb_core_metrics() -> Dict[str, List[str]]:
    """
    è·å–TiDBæœåŠ¡çš„æ ¸å¿ƒæŒ‡æ ‡åˆ—è¡¨ï¼ˆåŸºäºæ‚¨çš„ç­›é€‰å»ºè®®ï¼‰

    è¿”å›:
        æœåŠ¡ååˆ°æ ¸å¿ƒæŒ‡æ ‡åˆ—è¡¨çš„æ˜ å°„å­—å…¸
    """
    return {
        'tidb-tidb': [
            'failed_query_ops',  # å¤±è´¥è¯·æ±‚æ•° - é”™è¯¯ç‡æŒ‡æ ‡
            'duration_99th',  # 99åˆ†ä½è¯·æ±‚å»¶è¿Ÿ - å…³é”®æ€§èƒ½æŒ‡æ ‡
            'connection_count',  # è¿æ¥æ•° - è´Ÿè½½æŒ‡æ ‡
            'server_is_up',  # æœåŠ¡å­˜æ´»èŠ‚ç‚¹æ•° - å¯ç”¨æ€§æŒ‡æ ‡
            'cpu_usage',  # CPUä½¿ç”¨ç‡ - èµ„æºé¥±å’Œåº¦
            'memory_usage'  # å†…å­˜ä½¿ç”¨é‡ - èµ„æºä½¿ç”¨
        ],
        'tidb-pd': [
            'store_up_count',  # å¥åº·Storeæ•°é‡ - é›†ç¾¤å¥åº·åº¦
            'store_down_count',  # Down Storeæ•°é‡ - æ•…éšœæŒ‡æ ‡
            'store_unhealth_count',  # Unhealth Storeæ•°é‡ - å¼‚å¸¸æŒ‡æ ‡
            'storage_used_ratio',  # å·²ç”¨å®¹é‡æ¯” - å®¹é‡æŒ‡æ ‡
            'cpu_usage',  # CPUä½¿ç”¨ç‡ - èµ„æºä½¿ç”¨
            'memory_usage'  # å†…å­˜ä½¿ç”¨é‡ - èµ„æºä½¿ç”¨
        ],
        'tidb-tikv': [
            'cpu_usage',  # CPUä½¿ç”¨ç‡ - èµ„æºä½¿ç”¨
            'memory_usage',  # å†…å­˜ä½¿ç”¨é‡ - èµ„æºä½¿ç”¨
            'server_is_up',  # æœåŠ¡å­˜æ´»èŠ‚ç‚¹æ•° - å¯ç”¨æ€§
            'available_size',  # å¯ç”¨å­˜å‚¨å®¹é‡ - å®¹é‡é¢„è­¦
            'raft_propose_wait',  # RaftProposeç­‰å¾…å»¶è¿ŸP99 - æ€§èƒ½æŒ‡æ ‡
            'raft_apply_wait',  # RaftApplyç­‰å¾…å»¶è¿ŸP99 - æ€§èƒ½æŒ‡æ ‡
            'rocksdb_write_stall'  # RocksDBå†™é˜»å¡æ¬¡æ•° - å…³é”®å¼‚å¸¸æŒ‡æ ‡
        ]
    }


def load_tidb_service_data(date: str, service_name: str, metric_name: str) -> Optional[pd.DataFrame]:
    """
    åŠ è½½æŒ‡å®šTiDBæœåŠ¡çš„æŒ‡æ ‡æ•°æ®

    å‚æ•°:
        date: æ—¥æœŸï¼Œæ ¼å¼å¦‚ "2025-06-06"
        service_name: æœåŠ¡åç§°ï¼Œå¦‚ "tidb-tidb"
        metric_name: æŒ‡æ ‡åç§°ï¼Œå¦‚ "cpu_usage"

    è¿”å›:
        TiDBæœåŠ¡æŒ‡æ ‡æ•°æ®DataFrameï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›None
    """
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    # è·å–ç›®å½•æ˜ å°„
    directories = get_tidb_services_directories()
    if service_name not in directories:
        print(f"æœªçŸ¥çš„TiDBæœåŠ¡åç§°: {service_name}")
        return None

    # æ„å»ºæ•°æ®ç›®å½•è·¯å¾„
    data_dir = os.path.join(project_root, 'data', 'processed', f'{date}', 'metric-parquet', directories[service_name])

    # è·å–æ–‡ä»¶æ˜ å°„
    file_mapping = get_tidb_services_files_mapping(date)
    if service_name not in file_mapping or metric_name not in file_mapping[service_name]:
        print(f"æœªæ‰¾åˆ°æœåŠ¡ {service_name} çš„æŒ‡æ ‡ {metric_name} çš„æ–‡ä»¶æ˜ å°„")
        return None

    file_path = os.path.join(data_dir, file_mapping[service_name][metric_name])

    try:
        if not os.path.exists(file_path):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None

        df = pd.read_parquet(file_path)

        if len(df) == 0:
            print(f"æ–‡ä»¶ {file_path} ä¸­æ— æ•°æ®")
            return None

        return df

    except Exception as e:
        print(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        return None


def get_tidb_metrics_description_with_time_filter(df_tidb: pd.DataFrame, start_time: str, end_time: str,
                                                  metric_column: str, remove_outliers: bool = False) -> Optional[
    pd.Series]:
    """
    è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…TiDBæŒ‡æ ‡çš„æè¿°ç»Ÿè®¡

    å‚æ•°:
        df_tidb: TiDBæŒ‡æ ‡æ•°æ®DataFrame
        start_time: å¼€å§‹æ—¶é—´æˆ³
        end_time: ç»“æŸæ—¶é—´æˆ³
        metric_column: æŒ‡æ ‡åˆ—åï¼ˆå®é™…æ•°å€¼åˆ—ï¼‰
        remove_outliers: æ˜¯å¦ç§»é™¤å¼‚å¸¸å€¼

    è¿”å›:
        æŒ‡æ ‡æè¿°ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚æœæ— æ•°æ®åˆ™è¿”å›None
    """
    if 'timestamp_ns' not in df_tidb.columns:
        print("è­¦å‘Š: æœªæ‰¾åˆ°timestamp_nsåˆ—ï¼Œæ— æ³•è¿›è¡Œæ—¶é—´è¿‡æ»¤")
        return None

    # æ—¶é—´è¿‡æ»¤
    start_ts = int(start_time)
    end_ts = int(end_time)
    df_filtered = df_tidb[(df_tidb['timestamp_ns'] >= start_ts) & (df_tidb['timestamp_ns'] <= end_ts)]

    if len(df_filtered) == 0:
        print("æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ— æ•°æ®")
        return None

    # è·å–æŒ‡æ ‡æ•°æ®
    if metric_column not in df_filtered.columns:
        print(f"è­¦å‘Š: åˆ— '{metric_column}' ä¸å­˜åœ¨äºDataFrameä¸­")
        return None

    metric_data = df_filtered[metric_column].dropna()

    if len(metric_data) == 0:
        print(f"æŒ‡æ ‡ '{metric_column}' æ— æœ‰æ•ˆæ•°æ®")
        return None

    # æ˜¯å¦ç§»é™¤å¼‚å¸¸å€¼
    if remove_outliers and len(metric_data) > 4:
        metric_data_sorted = metric_data.sort_values()
        metric_data = metric_data_sorted.iloc[2:-2]  # å»æ‰æœ€å°2ä¸ªå’Œæœ€å¤§2ä¸ª
        print(f"ç§»é™¤å¼‚å¸¸å€¼åæ•°æ®ç‚¹æ•°é‡: {len(metric_data)}")
    desc = metric_data.describe(percentiles=[0.25, 0.5, 0.75, 0.95, 0.99])

    # **æ–°å¢éé›¶æ¯”ä¾‹**
    desc['non_zero_ratio'] = round((metric_data != 0).sum() / len(metric_data), 3)

    return desc


def analyze_tidb_services_metrics(df_fault_timestamps: pd.DataFrame, index: int) -> Optional[Dict]:
    """
    åˆ†æTiDBæœåŠ¡åœ¨æ•…éšœæ—¶é—´æ®µä¸æ­£å¸¸æ—¶é—´æ®µçš„æŒ‡æ ‡å¯¹æ¯”
    ç»“æ„ï¼šservice â†’ metric â†’ {normal_periods_combined, fault_period}

    å‚æ•°:
        df_fault_timestamps: æ•…éšœæ—¶é—´æˆ³DataFrame
        index: è¦åˆ†æçš„æ•…éšœç´¢å¼•

    è¿”å›:
        æŒ‰TiDBæœåŠ¡ç»„ç»‡çš„åŒ…å«æ•…éšœå’Œæ­£å¸¸æ—¶é—´æ®µæŒ‡æ ‡å¯¹æ¯”çš„å­—å…¸
    """
    # è·å–æ•…éšœæ—¶é—´ä¿¡æ¯
    _, date, fault_start, fault_end = get_fault_period_info(df_fault_timestamps, index)
    normal_periods = get_normal_time_periods(df_fault_timestamps, index)

    # è·å–TiDBæœåŠ¡å’Œæ ¸å¿ƒæŒ‡æ ‡
    core_metrics = get_tidb_core_metrics()

    print(f"TiDBæœåŠ¡åˆ†æ - æ•…éšœæ—¥æœŸ: {date}")
    print(f"TiDBæœåŠ¡åˆ†æ - æ•…éšœæ—¶é—´æ®µ: {fault_start} è‡³ {fault_end}")
    print(f"TiDBæœåŠ¡åˆ†æ - æ­£å¸¸æ—¶é—´æ®µæ•°é‡: {len(normal_periods)}")
    print(f"TiDBæœåŠ¡åˆ†æ - åˆ†ææœåŠ¡æ•°é‡: {len(core_metrics)}")

    # æŒ‰ æœåŠ¡ â†’ æŒ‡æ ‡ â†’ æ—¶é—´æ®µ ç»“æ„ç»„ç»‡åˆ†æç»“æœ
    tidb_analysis = {}

    for service_name, metrics_list in core_metrics.items():
        print(f"\n=== å¤„ç†TiDBæœåŠ¡: {service_name} ===")

        # åˆå§‹åŒ–æœåŠ¡ç»“æ„
        tidb_analysis[service_name] = {}

        for metric_name in metrics_list:
            print(f"  å¤„ç†æŒ‡æ ‡: {metric_name}")

            # åŠ è½½è¯¥æŒ‡æ ‡çš„æ•°æ®
            df_metric = load_tidb_service_data(date, service_name, metric_name)

            if df_metric is None:
                print(f"    æ— æ³•åŠ è½½æŒ‡æ ‡ {metric_name} çš„æ•°æ®ï¼Œè·³è¿‡")
                continue

            # åˆå§‹åŒ–æŒ‡æ ‡ç»“æ„
            tidb_analysis[service_name][metric_name] = {
                'normal_periods_combined': None,
                'fault_period': None
            }

            # 1. åˆå¹¶æ‰€æœ‰æ­£å¸¸æ—¶é—´æ®µæ•°æ®è¿›è¡Œç»Ÿè®¡
            print(f"    æ­£å¸¸æ—¶é—´æ®µåˆ†æ:")
            all_normal_data = []

            for i, (normal_start, normal_end) in enumerate(normal_periods):
                start_ts = int(normal_start)
                end_ts = int(normal_end)
                normal_data = df_metric[(df_metric['timestamp_ns'] >= start_ts) & (df_metric['timestamp_ns'] <= end_ts)]

                if len(normal_data) > 0:
                    all_normal_data.append(normal_data)
                    print(f"      æ—¶é—´æ®µ {i + 1} æ•°æ®è¡Œæ•°: {len(normal_data)}")

            # åˆå¹¶æ­£å¸¸æ—¶é—´æ®µæ•°æ®å¹¶ç»Ÿè®¡
            if all_normal_data:
                combined_normal_data = pd.concat(all_normal_data, ignore_index=True)
                print(f"    åˆå¹¶åæ­£å¸¸æ—¶é—´æ®µæ€»æ•°æ®è¡Œæ•°: {len(combined_normal_data)}")

                # è·å–ç»Ÿè®¡ï¼ˆç§»é™¤å¼‚å¸¸å€¼ï¼‰
                normal_desc = get_tidb_metrics_description_with_time_filter(
                    combined_normal_data,
                    str(combined_normal_data['timestamp_ns'].min()),
                    str(combined_normal_data['timestamp_ns'].max()),
                    metric_name,
                    remove_outliers=(len(combined_normal_data) > 4)
                )

                tidb_analysis[service_name][metric_name]['normal_periods_combined'] = normal_desc

                if normal_desc is not None:
                    print(f"    æ­£å¸¸æœŸé—´ {metric_name}: mean={normal_desc['mean']:.2f}, std={normal_desc['std']:.2f}")

            # 2. æ•…éšœæ—¶é—´æ®µç»Ÿè®¡
            print(f"    æ•…éšœæ—¶é—´æ®µåˆ†æ:")
            fault_desc = get_tidb_metrics_description_with_time_filter(
                df_metric, fault_start, fault_end, metric_name, remove_outliers=False
            )

            tidb_analysis[service_name][metric_name]['fault_period'] = fault_desc

            if fault_desc is not None:
                fault_data_count = len(df_metric[(df_metric['timestamp_ns'] >= int(fault_start)) &
                                                 (df_metric['timestamp_ns'] <= int(fault_end))])
                print(f"    æ•…éšœæ—¶é—´æ®µæ•°æ®è¡Œæ•°: {fault_data_count}")
                print(f"    æ•…éšœæœŸé—´ {metric_name}: mean={fault_desc['mean']:.2f}, std={fault_desc['std']:.2f}")

    return tidb_analysis if tidb_analysis else None


def create_combined_service_prompt_with_tidb(service_analysis_results: Dict, tidb_analysis_results: Dict = None) -> str:
    """
    åˆå¹¶æ‰€æœ‰æœåŠ¡ï¼ˆåŒ…æ‹¬TiDBæœåŠ¡ï¼‰çš„promptä¸ºä¸€ä¸ªç»¼åˆprompt

    å‚æ•°:
        service_analysis_results: æ™®é€šæœåŠ¡åˆ†æç»“æœ
        tidb_analysis_results: TiDBæœåŠ¡åˆ†æç»“æœ

    è¿”å›:
        åˆå¹¶åçš„ç»¼åˆprompt
    """
    if not service_analysis_results and not tidb_analysis_results:
        return ""

    # æ„å»ºåˆå¹¶çš„æ•°æ®å¯¹æ¯”è¡¨æ ¼
    combined_json = {}

    # 1. å¤„ç†æ™®é€šæœåŠ¡
    if service_analysis_results:
        for service_name, service_data in service_analysis_results.items():
            combined_json[service_name] = {
                "service_name": service_name,
                "service_type": "microservice",  # æ ‡è®°ä¸ºå¾®æœåŠ¡
                "pod_count": len(service_data),
                "pod_list": list(service_data.keys()),
                "pods": {}
            }

            for pod_name, pod_metrics in service_data.items():
                normal_stats = pod_metrics.get('normal_periods_combined', {})
                fault_stats = pod_metrics.get('fault_period', {})

                pod_json = {}
                target_columns = ['client_error_ratio', 'error_ratio', 'request', 'response',
                                  'rrt', 'server_error_ratio', 'timeout']

                for metric in target_columns:
                    normal_desc = normal_stats.get(metric)
                    fault_desc = fault_stats.get(metric)

                    if normal_desc is not None and fault_desc is not None:
                        normal_mean = normal_desc.get('mean', 0)
                        normal_std = normal_desc.get('std', 0)
                        normal_max = normal_desc.get('max', 0)
                        normal_p99 = normal_desc.get('99%', 0)
                        normal_p50 = normal_desc.get('50%', 0)
                        normal_p25 = normal_desc.get('25%', 0)
                        normal_p75 = normal_desc.get('75%', 0)
                        fault_mean = fault_desc.get('mean', 0)
                        fault_std = fault_desc.get('std', 0)
                        fault_max = fault_desc.get('max', 0)
                        fault_p99 = fault_desc.get('99%', 0)
                        fault_p50 = fault_desc.get('50%', 0)
                        fault_p25 = fault_desc.get('25%', 0)
                        fault_p75 = fault_desc.get('75%', 0)
                        normal_iqr = normal_p75 - normal_p25
                        fault_iqr = fault_p75 - fault_p25
                        fault_nzr = fault_desc.get('non_zero_ratio', 0)
                        normal_nzr = normal_desc.get('non_zero_ratio', 0)
                        # è®¡ç®—å¯¹ç§°æ¯”ç‡
                        p50_symmetric_ratio = abs(fault_p50 - normal_p50) / (
                                (fault_p50 + normal_p50) / 2 + 1e-9
                        )
                        p99_symmetric_ratio = abs(fault_p99 - normal_p99) / (
                                (fault_p99 + normal_p99) / 2 + 1e-9
                        )
                        # è¿‡æ»¤é˜ˆå€¼ï¼ˆå»ºè®® 0.5 æˆ– 1ï¼Œæ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
                        if p50_symmetric_ratio < 0.05 and p99_symmetric_ratio < 0.05:
                            continue

                        pod_json[metric] = {
                            "æ­£å¸¸æœŸé—´ä¸­ä½æ•°": round(normal_p50, 2),
                            "æ­£å¸¸æœŸé—´å››åˆ†ä½è·": round(normal_iqr, 2),
                            "æ­£å¸¸æœŸé—´99åˆ†ä½æ•°": round(normal_p99, 2),
                            "æ•…éšœæœŸé—´ä¸­ä½æ•°": round(fault_p50, 2),
                            "æ•…éšœæœŸé—´å››åˆ†ä½è·": round(fault_iqr, 2),
                            "æ•…éšœæœŸé—´99åˆ†ä½æ•°": round(fault_p99, 2)
                        }

                combined_json[service_name]["pods"][pod_name] = pod_json

    # 2. å¤„ç†TiDBæœåŠ¡
    if tidb_analysis_results:
        for service_name, service_metrics in tidb_analysis_results.items():
            combined_json[service_name] = {
                "service_name": service_name,
                "service_type": "tidb_component",  # æ ‡è®°ä¸ºTiDBç»„ä»¶
                "metrics": {}  # TiDBæœåŠ¡ç›´æ¥å­˜å‚¨æŒ‡æ ‡ï¼Œæ²¡æœ‰Podæ¦‚å¿µ
            }

            for metric_name, metric_stats in service_metrics.items():
                normal_stats = metric_stats.get('normal_periods_combined')
                fault_stats = metric_stats.get('fault_period')

                if normal_stats is not None and fault_stats is not None:
                    normal_mean = normal_stats.get('mean', 0)
                    normal_std = normal_stats.get('std', 0)
                    normal_max = normal_stats.get('max', 0)
                    normal_p99 = normal_stats.get('99%', 0)
                    normal_p50 = normal_stats.get('50%', 0)
                    normal_p25 = normal_stats.get('25%', 0)
                    normal_p75 = normal_stats.get('75%', 0)

                    fault_mean = fault_stats.get('mean', 0)
                    fault_std = fault_stats.get('std', 0)
                    fault_max = fault_stats.get('max', 0)
                    fault_p99 = fault_stats.get('99%', 0)
                    fault_p50 = fault_stats.get('50%', 0)
                    fault_p25 = fault_stats.get('25%', 0)
                    fault_p75 = fault_stats.get('75%', 0)
                    normal_iqr = normal_p75 - normal_p25
                    fault_iqr = fault_p75 - fault_p25
                    fault_nzr = fault_stats.get('non_zero_ratio', 0)
                    normal_nzr = normal_stats.get('non_zero_ratio', 0)
                    # è®¡ç®—å¯¹ç§°æ¯”ç‡
                    p50_symmetric_ratio = abs(fault_p50 - normal_p50) / (
                            (fault_p50 + normal_p50) / 2 + 1e-9
                    )
                    p99_symmetric_ratio = abs(fault_p99 - normal_p99) / (
                            (fault_p99 + normal_p99) / 2 + 1e-9
                    )
                    # è¿‡æ»¤é˜ˆå€¼ï¼ˆå»ºè®® 0.5 æˆ– 1ï¼Œæ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
                    if p50_symmetric_ratio < 0.05 and p99_symmetric_ratio < 0.05:
                        continue

                    combined_json[service_name]["metrics"][metric_name] = {
                        "æ­£å¸¸æœŸé—´ä¸­ä½æ•°": round(normal_p50, 2),
                        "æ­£å¸¸æœŸé—´å››åˆ†ä½è·": round(normal_iqr, 2),
                        "æ­£å¸¸æœŸé—´99åˆ†ä½æ•°": round(normal_p99, 2),
                        "æ•…éšœæœŸé—´ä¸­ä½æ•°": round(fault_p50, 2),
                        "æ•…éšœæœŸé—´å››åˆ†ä½è·": round(fault_iqr, 2),
                        "æ•…éšœæœŸé—´99åˆ†ä½æ•°": round(fault_p99, 2)
                    }
                else:
                    combined_json[service_name]["metrics"][metric_name] = "ç¼ºå¤±æ•°æ®"

    # ç»Ÿè®¡ä¿¡æ¯
    all_services = list(combined_json.keys())
    microservice_count = sum(1 for s in combined_json.values() if s.get("service_type") == "microservice")
    tidb_service_count = sum(1 for s in combined_json.values() if s.get("service_type") == "tidb_component")
    total_pods = sum(s.get("pod_count", 0) for s in combined_json.values() if "pod_count" in s)

    return f"""
è¯·æ ¹æ®æä¾›çš„APMï¼ˆåº”ç”¨æ€§èƒ½ç›‘æ§ï¼‰æŒ‡æ ‡æ•°æ®å’ŒTiDBåˆ†å¸ƒå¼æ•°æ®åº“æŒ‡æ ‡æ•°æ®ï¼Œæè¿°æ‰€æœ‰æœåŠ¡åœ¨æ­£å¸¸æœŸé—´å’Œæ•…éšœæœŸé—´çš„ä¸šåŠ¡æœåŠ¡æ€§èƒ½è¡¨ç°å·®å¼‚ç°è±¡ã€‚

## ç³»ç»Ÿæ•´ä½“ä¿¡æ¯
- **å¾®æœåŠ¡æ€»æ•°**: {microservice_count}
- **TiDBç»„ä»¶æ•°**: {tidb_service_count}
- **æœåŠ¡æ€»æ•°**: {len(all_services)}
- **Podæ€»æ•°**: {total_pods}
- **æœåŠ¡åˆ—è¡¨**: {all_services}

## APMå…³é”®æŒ‡æ ‡è¯´æ˜ï¼ˆå¾®æœåŠ¡ï¼‰
### è¯·æ±‚å“åº”ç±»æŒ‡æ ‡ï¼š
- `request`: è¯·æ±‚æ•°é‡ - åæ˜ æœåŠ¡æ¥æ”¶åˆ°çš„ä¸šåŠ¡è¯·æ±‚æ€»æ•°
- `response`: å“åº”æ•°é‡ - åæ˜ æœåŠ¡æˆåŠŸå¤„ç†å¹¶å“åº”çš„è¯·æ±‚æ€»æ•°
- `rrt`: å¹³å‡æ—¶å»¶ - åæ˜ æœåŠ¡å¤„ç†è¯·æ±‚çš„å¹³å‡å“åº”æ—¶é—´

### å¼‚å¸¸ç±»æŒ‡æ ‡ï¼š
- `timeout`: è¶…æ—¶æ•°é‡ - åæ˜ æœåŠ¡å¤„ç†è¯·æ±‚è¶…æ—¶çš„æ¬¡æ•°
- `error_ratio`: å¼‚å¸¸æ¯”ä¾‹ - å¼‚å¸¸è¯·æ±‚å æ€»è¯·æ±‚çš„æ¯”ä¾‹
- `client_error_ratio`: å®¢æˆ·ç«¯å¼‚å¸¸æ¯”ä¾‹ - å®¢æˆ·ç«¯å¼‚å¸¸å æ€»è¯·æ±‚çš„æ¯”ä¾‹
- `server_error_ratio`: æœåŠ¡ç«¯å¼‚å¸¸æ¯”ä¾‹ - æœåŠ¡ç«¯å¼‚å¸¸å æ€»è¯·æ±‚çš„æ¯”ä¾‹

## TiDBå…³é”®æŒ‡æ ‡è¯´æ˜ï¼ˆæ•°æ®åº“ç»„ä»¶ï¼‰
### TiDBç»„ä»¶æŒ‡æ ‡ï¼š
- `failed_query_ops`: å¤±è´¥è¯·æ±‚æ•° - æ•°æ®åº“è¯·æ±‚é”™è¯¯ç‡æŒ‡æ ‡
- `duration_99th`: 99åˆ†ä½è¯·æ±‚å»¶è¿Ÿ - æ•°æ®åº“å…³é”®æ€§èƒ½æŒ‡æ ‡
- `connection_count`: è¿æ¥æ•° - æ•°æ®åº“è´Ÿè½½æŒ‡æ ‡
- `server_is_up`: æœåŠ¡å­˜æ´»èŠ‚ç‚¹æ•° - æ•°æ®åº“å¯ç”¨æ€§æŒ‡æ ‡
- `cpu_usage`: CPUä½¿ç”¨ç‡ - æ•°æ®åº“èµ„æºé¥±å’Œåº¦
- `memory_usage`: å†…å­˜ä½¿ç”¨é‡ - æ•°æ®åº“èµ„æºä½¿ç”¨

### TiKVç»„ä»¶æŒ‡æ ‡ï¼š
- `cpu_usage`: CPUä½¿ç”¨ç‡ - å­˜å‚¨å±‚èµ„æºä½¿ç”¨
- `memory_usage`: å†…å­˜ä½¿ç”¨é‡ - å­˜å‚¨å±‚èµ„æºä½¿ç”¨
- `server_is_up`: æœåŠ¡å­˜æ´»èŠ‚ç‚¹æ•° - å­˜å‚¨å±‚å¯ç”¨æ€§
- `available_size`: å¯ç”¨å­˜å‚¨å®¹é‡ - å­˜å‚¨å®¹é‡é¢„è­¦
- `raft_propose_wait`: RaftProposeç­‰å¾…å»¶è¿ŸP99 - åˆ†å¸ƒå¼ä¸€è‡´æ€§æ€§èƒ½
- `raft_apply_wait`: RaftApplyç­‰å¾…å»¶è¿ŸP99 - åˆ†å¸ƒå¼ä¸€è‡´æ€§æ€§èƒ½
- `rocksdb_write_stall`: RocksDBå†™é˜»å¡æ¬¡æ•° - å­˜å‚¨å¼•æ“å¼‚å¸¸æŒ‡æ ‡

### PDç»„ä»¶æŒ‡æ ‡ï¼š
- `store_up_count`: å¥åº·Storeæ•°é‡ - é›†ç¾¤å¥åº·åº¦
- `store_down_count`: Down Storeæ•°é‡ - é›†ç¾¤æ•…éšœæŒ‡æ ‡
- `store_unhealth_count`: Unhealth Storeæ•°é‡ - é›†ç¾¤å¼‚å¸¸æŒ‡æ ‡
- `storage_used_ratio`: å·²ç”¨å®¹é‡æ¯” - é›†ç¾¤å®¹é‡æŒ‡æ ‡
- `cpu_usage`: CPUä½¿ç”¨ç‡ - è°ƒåº¦å™¨èµ„æºä½¿ç”¨
- `memory_usage`: å†…å­˜ä½¿ç”¨é‡ - è°ƒåº¦å™¨èµ„æºä½¿ç”¨

## æ•°æ®å¯¹æ¯”è¡¨æ ¼,ç‰¹åˆ«æ³¨æ„**ç¼ºå¤±æ•°æ®å’Œç©ºæ•°æ®,ä»£è¡¨æ•°æ®æ³¢åŠ¨æå°,é€šå¸¸æƒ…å†µé»˜è®¤æ­£å¸¸**:
{json.dumps(combined_json, ensure_ascii=False, indent=2)}

## ç°è±¡æè¿°è¦æ±‚
è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œç°è±¡æè¿°ï¼Œ**ä»…æè¿°è§‚å¯Ÿåˆ°çš„ç°è±¡ï¼Œä¸åšå¼‚å¸¸åˆ¤æ–­æˆ–ç»“è®º**ï¼š

### å¾®æœåŠ¡çº§åˆ«ç°è±¡è§‚å¯Ÿ
- é›†ä¸­åœ¨åŒä¸€ç±»å‹çš„å¾®æœåŠ¡ä¸­å‡ºç°çš„é—®é¢˜ï¼Œæ¯”å¦‚emailservice-0, emailservice-1, emailservice-2, éƒ½å­˜åœ¨ç›¸ä¼¼çš„å¼‚å¸¸æ•°æ®å˜åŒ–
- ä¸ªåˆ«Podçš„å¼‚å¸¸è¡¨ç°ç‰¹å¾ï¼Œæ¯”å¦‚cartservice-0ä¸­å­˜åœ¨å¼‚å¸¸çš„æ•°æ®å˜åŒ–ï¼Œè€Œcartservice-1, cartservice-2ä»¥åŠå…¶ä»–podæ­£å¸¸

### TiDBæ•°æ®åº“ç»„ä»¶ç°è±¡è§‚å¯Ÿ
- TiDBç»„ä»¶ï¼ˆtidb-tidbï¼‰ã€TiKVç»„ä»¶ï¼ˆtidb-tikvï¼‰å’Œ PDç»„ä»¶ï¼ˆtidb-pdï¼‰çš„å¼‚å¸¸æ•°æ®å˜åŒ–

## é‡è¦æç¤º
**è¿™æ˜¯ä¸ºåæœŸç»¼åˆå†³ç­–åˆ†ææä¾›çš„ç³»ç»Ÿçº§ç°è±¡æ€»ç»“ï¼Œè¯·æ§åˆ¶æ€»ç»“å†…å®¹åœ¨2000å­—å·¦å³ï¼Œé‡ç‚¹çªå‡ºä¸»è¦å˜åŒ–ç°è±¡ã€‚**

**æ€»ç»“è¦æ±‚ï¼š**
- å¦‚æœæ•´ä½“è¡¨ç°æ­£å¸¸ç¨³å®šï¼Œè¯·ç®€è¦è¯´æ˜"ç³»ç»Ÿå„é¡¹æŒ‡æ ‡è¡¨ç°ç¨³å®šï¼Œæœªè§‚å¯Ÿåˆ°æ˜¾è‘—å˜åŒ–ç°è±¡"
- å¦‚æœå‡ºç°æ˜æ˜¾å˜åŒ–ï¼Œè¯·é‡ç‚¹æè¿°å˜åŒ–æ˜¾è‘—çš„æœåŠ¡ã€æŒ‡æ ‡å’Œç°è±¡
- é‡‡ç”¨æ¦‚æ‹¬æ€§è¯­è¨€ï¼Œé‡ç‚¹å…³æ³¨å¯¹ä¸šåŠ¡å½±å“è¾ƒå¤§çš„æŒ‡æ ‡å˜åŒ–
- ç¼ºå¤±æˆ–ä¸ºç©ºçš„æ•°æ®è¡¨ç¤ºæ³¢åŠ¨æå°ï¼Œå¯è§†ä¸ºæ­£å¸¸ï¼Œæ— éœ€æè¿°

è¯·åŸºäºAPMä¸šåŠ¡ç›‘æ§æ•°æ®å’ŒTiDBæ•°æ®åº“ç›‘æ§æ•°æ®å®¢è§‚æè¿°è§‚å¯Ÿåˆ°çš„ç°è±¡ï¼Œæ§åˆ¶åœ¨2000å­—ä»¥å†…ï¼Œä¸ºåç»­ç»¼åˆåˆ†ææä¾›ç®€æ´æœ‰æ•ˆçš„ç°è±¡æ€»ç»“ã€‚
"""


def analyze_fault_comprehensive(df_fault_timestamps: pd.DataFrame, index: int, uuid: str = None) -> str:
    """
    å¯¹æŒ‡å®šæ•…éšœè¿›è¡Œç»¼åˆåˆ†æï¼ŒåŒ…æ‹¬Serviceçº§åˆ«ã€TiDBæœåŠ¡å’ŒNodeçº§åˆ«çš„å®Œæ•´åˆ†ææµç¨‹
    æ³¨æ„ï¼šTiDBæœåŠ¡æ•°æ®ä¼šç›´æ¥æ•´åˆåˆ°ç¬¬ä¸€æ¬¡Serviceåˆ†æä¸­ï¼Œæ€»å…±åªè°ƒç”¨2æ¬¡LLM

    å‚æ•°:
        df_fault_timestamps: æ•…éšœæ—¶é—´æˆ³DataFrame
        index: è¦åˆ†æçš„æ•…éšœç´¢å¼•
        uuid: æ ·æœ¬UUIDï¼Œç”¨äºè®°å½•å¤§æ¨¡å‹è°ƒç”¨

    è¿”å›:
        node_analysis_result: ç»¼åˆNodeçº§åˆ«åˆ†æç»“æœï¼ˆåŒ…å«Serviceå’ŒTiDBåˆ†æç»“æœï¼‰
    """
    # å®šä¹‰è¦åˆ†æçš„å…³é”®æŒ‡æ ‡åˆ—
    key_metrics = ['client_error_ratio', 'error_ratio', 'request', 'response', 'rrt', 'server_error_ratio', 'timeout']

    # å®šä¹‰è¦åˆ†æçš„èŠ‚ç‚¹æŒ‡æ ‡
    node_metrics = ['node_cpu_usage_rate',
                    'node_disk_read_bytes_total',
                    'node_disk_read_time_seconds_total',
                    'node_disk_write_time_seconds_total',
                    'node_disk_written_bytes_total',
                    'node_filesystem_free_bytes',
                    'node_filesystem_usage_rate',
                    'node_memory_MemAvailable_bytes',
                    'node_memory_MemTotal_bytes',
                    'node_memory_usage_rate',
                    'node_network_receive_bytes_total',
                    'node_network_receive_packets_total',
                    'node_network_transmit_bytes_total',
                    'node_network_transmit_packets_total',
                    'node_sockstat_TCP_inuse']

    pod_metrics = [
        'pod_cpu_usage', 'pod_fs_reads_bytes', 'pod_fs_writes_bytes',
        'pod_memory_working_set_bytes', 'pod_network_receive_bytes',
        'pod_network_receive_packets', 'pod_network_transmit_bytes',
        'pod_network_transmit_packets', 'pod_processes'
    ]

    try:
        # è·å–å½“å‰æ•…éšœçš„æ—¥æœŸ
        fault_date = df_fault_timestamps.iloc[index]['date']

        print(f"å¼€å§‹åˆ†ææ•…éšœç´¢å¼•: {index}")
        print("=" * 80)

        # ==================== SERVICEçº§åˆ«åˆ†æï¼ˆåŒ…å«TiDBæœåŠ¡ï¼‰====================
        print(f"\n{'ğŸ”¹' * 40}")
        print("SERVICEå’ŒTiDBæœåŠ¡çº§åˆ«æ•°æ®åˆ†æï¼ˆä¸€æ¬¡æ€§å¤„ç†ï¼‰")
        print(f"{'ğŸ”¹' * 40}")

        # 1. åˆ†ææ™®é€šå¾®æœåŠ¡
        service_result = analyze_fault_vs_normal_metrics_by_service(df_fault_timestamps, index, key_metrics)
        if service_result is None:
            print("æœªæ‰¾åˆ°åŒ¹é…çš„ServiceæŒ‡æ ‡æ•°æ®")
            service_result = {}
        else:
            print(f"æˆåŠŸåˆ†æäº† {len(service_result)} ä¸ªService")

        # 2. åˆ†æTiDBæœåŠ¡ï¼ˆç›´æ¥æ•´åˆï¼‰
        tidb_result = analyze_tidb_services_metrics(df_fault_timestamps, index)
        if tidb_result is None:
            print("æœªæ‰¾åˆ°åŒ¹é…çš„TiDBæœåŠ¡æŒ‡æ ‡æ•°æ®")
            tidb_result = {}
        else:
            print(f"æˆåŠŸåˆ†æäº† {len(tidb_result)} ä¸ªTiDBæœåŠ¡")

        # 3. åˆ›å»ºåˆå¹¶çš„Service+TiDB promptå¹¶è°ƒç”¨LLMï¼ˆç¬¬1æ¬¡è°ƒç”¨ï¼‰
        combined_service_prompt = create_combined_service_prompt_with_tidb(service_result, tidb_result)
        print("å·²ç”Ÿæˆåˆå¹¶çš„Serviceå’ŒTiDBåˆ†æprompt")

        print("æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹åˆ†æServiceå’ŒTiDBæ•°æ®ï¼ˆç¬¬1æ¬¡è°ƒç”¨ï¼‰...")
        service_analysis_result = call_llm_analysis(combined_service_prompt, uuid,
                                                    "ç¬¬1æ¬¡è°ƒç”¨-Serviceå’ŒTiDBçº§åˆ«ç»¼åˆåˆ†æ")
        print("Serviceå’ŒTiDBçº§åˆ«åˆ†æå®Œæˆ")

        # ==================== è·å–Podéƒ¨ç½²ä¿¡æ¯ ====================
        print(f"\n{'ğŸ“‹' * 40}")
        print("è·å–Podéƒ¨ç½²ä¿¡æ¯")
        print(f"{'ğŸ“‹' * 40}")

        node_pod_mapping = get_node_pod_mapping(fault_date)
        if node_pod_mapping:
            total_pods = sum(len(pods) for pods in node_pod_mapping.values())
            print(f"æˆåŠŸè·å–Podéƒ¨ç½²ä¿¡æ¯ï¼Œæ€»Podæ•°: {total_pods}")
            for node_name, pods in node_pod_mapping.items():
                print(f"  {node_name}: {len(pods)} ä¸ªPod")
        else:
            print("æœªèƒ½è·å–Podéƒ¨ç½²ä¿¡æ¯ï¼Œå°†ä½¿ç”¨ç©ºçš„éƒ¨ç½²æ˜ å°„")

        # ==================== NODEçº§åˆ«åˆ†æ ====================
        print(f"\n{'ğŸ”·' * 40}")
        print("NODEçº§åˆ«æ•°æ®åˆ†æ")
        print(f"{'ğŸ”·' * 40}")

        pod_result = analyze_pod_metrics_by_pod(df_fault_timestamps, index, pod_metrics)
        node_result = analyze_node_metrics_by_node(df_fault_timestamps, index, node_metrics)

        if node_result is None:
            print("æœªæ‰¾åˆ°åŒ¹é…çš„NodeæŒ‡æ ‡æ•°æ®")
            node_result = {}
        else:
            print(f"æˆåŠŸåˆ†æäº† {len(node_result)} ä¸ªNode")

        # åˆ›å»ºåŒ…å«Service+TiDBåˆ†æç»“æœçš„Node promptå¹¶è°ƒç”¨LLMï¼ˆç¬¬2æ¬¡è°ƒç”¨ï¼‰
        combined_node_prompt = create_combined_node_prompt_with_service_analysis(
            node_result, pod_result, service_analysis_result, node_pod_mapping)
        print("å·²ç”ŸæˆåŒ…å«Serviceå’ŒTiDBåˆ†æç»“æœçš„ç»¼åˆNodeåˆ†æprompt")

        print("æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œç»¼åˆNodeåˆ†æï¼ˆç¬¬2æ¬¡è°ƒç”¨ï¼‰...")
        node_analysis_result = call_llm_analysis(combined_node_prompt, uuid, "ç¬¬2æ¬¡è°ƒç”¨-Nodeçº§åˆ«ç»¼åˆåˆ†æ")
        print("ç»¼åˆNodeçº§åˆ«åˆ†æå®Œæˆ")

        print("=" * 80)
        print(f"æ•…éšœç´¢å¼• {index} ç»¼åˆåˆ†æå®Œæˆï¼ˆåŒ…å«TiDBæœåŠ¡ï¼Œæ€»è®¡2æ¬¡LLMè°ƒç”¨ï¼‰")
        print("=" * 80)

        return node_analysis_result

    except Exception as e:
        error_msg = f"åˆ†ææ•…éšœç´¢å¼• {index} æ—¶å‡ºé”™: {str(e)}"
        print(error_msg)
        raise Exception(error_msg)


# ==================== æ›´æ–°ä¸»å‡½æ•°ä»¥æ”¯æŒTiDB ====================

if __name__ == "__main__":
    # è·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    # ä½¿ç”¨ç»å¯¹è·¯å¾„æ„å»ºinput_timestamp.csvçš„è·¯å¾„
    input_path = os.path.join(project_root, 'input', 'input_timestamp.csv')
    df_fault_timestamps = pd.read_csv(input_path)

    # åˆ›å»ºoutputç›®å½•
    output_dir = os.path.join(project_root, 'output')
    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»ºæ—¶é—´æˆ³
    from datetime import datetime

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # é™åˆ¶åªå¤„ç†å‰2ç»„æ•°æ®è¿›è¡ŒéªŒè¯ï¼ˆåŒ…å«TiDBæœåŠ¡åˆ†æï¼‰
    MAX_PROCESS_COUNT = 2
    total_count = min(len(df_fault_timestamps), MAX_PROCESS_COUNT)

    # ç»Ÿè®¡å˜é‡
    successful_analysis_count = 0
    failed_analysis_count = 0

    print(f"å¼€å§‹å¤„ç†å‰{MAX_PROCESS_COUNT}ç»„æ•°æ®è¿›è¡Œç»¼åˆLLMåˆ†æï¼ˆåŒ…å«TiDBæœåŠ¡ï¼‰")
    print("=" * 100)

    for index, row in df_fault_timestamps.iterrows():
        # é™åˆ¶åªå¤„ç†å‰MAX_PROCESS_COUNTç»„æ•°æ®
        if index >= MAX_PROCESS_COUNT:
            print(f"å·²å¤„ç†å‰{MAX_PROCESS_COUNT}ç»„æ•°æ®ï¼Œåœæ­¢å¤„ç†")
            break

        print("=" * 100)
        print(f"å¤„ç†æ•…éšœç´¢å¼•: {index} (éªŒè¯æ¨¡å¼: {index + 1}/{MAX_PROCESS_COUNT}) - åŒ…å«TiDBæœåŠ¡åˆ†æ")
        print("=" * 100)

        try:
            # ä½¿ç”¨æ›´æ–°çš„åŒ…å«TiDBæœåŠ¡çš„ç»¼åˆåˆ†æå‡½æ•°
            node_analysis_result = analyze_fault_comprehensive(df_fault_timestamps, index, None)

            # ==================== ä¿å­˜ç»“æœ ====================
            print(f"\n{'ğŸ’¾' * 50}")
            print("ä¿å­˜åˆ†æç»“æœ")
            print(f"{'ğŸ’¾' * 50}")

            # åˆ›å»ºåˆ†æç»“æœç›®å½•
            analysis_output_dir = os.path.join(output_dir, 'llm_analysis')
            os.makedirs(analysis_output_dir, exist_ok=True)

            # ä¿å­˜ç»¼åˆåˆ†æç»“æœï¼ˆåŒ…å«TiDBï¼‰
            analysis_file_path = os.path.join(analysis_output_dir,
                                              f'fault_{index:03d}_comprehensive_with_tidb_analysis_{timestamp}.txt')
            with open(analysis_file_path, 'w', encoding='utf-8') as f:
                f.write(f"æ•…éšœç´¢å¼• {index} - ç»¼åˆåˆ†æç»“æœï¼ˆåŒ…å«TiDBæœåŠ¡ï¼‰\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {timestamp}\n")
                f.write(f"åˆ†æå†…å®¹: å¾®æœåŠ¡APM + TiDBæ•°æ®åº“ç»„ä»¶ + åŸºç¡€è®¾æ–½ç›‘æ§\n")
                f.write(f"{'=' * 80}\n\n")
                f.write(node_analysis_result)

            print(f"ç»¼åˆåˆ†æç»“æœå·²ä¿å­˜: {analysis_file_path}")

            successful_analysis_count += 1
            print(f"æ•…éšœç´¢å¼• {index} åˆ†æå®Œæˆå¹¶ä¿å­˜ï¼ˆåŒ…å«TiDBæœåŠ¡ï¼‰")

        except Exception as e:
            error_msg = f"å¤„ç†æ•…éšœç´¢å¼• {index} æ—¶å‡ºé”™: {e}"
            print(error_msg)
            failed_analysis_count += 1

            # ä¿å­˜é”™è¯¯ä¿¡æ¯
            error_file_path = os.path.join(output_dir, 'llm_analysis',
                                           f'fault_{index:03d}_error_with_tidb_{timestamp}.txt')
            os.makedirs(os.path.dirname(error_file_path), exist_ok=True)
            with open(error_file_path, 'w', encoding='utf-8') as f:
                f.write(f"æ•…éšœç´¢å¼• {index} å¤„ç†é”™è¯¯ï¼ˆTiDBåˆ†ææ¨¡å¼ï¼‰\n")
                f.write(f"é”™è¯¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"é”™è¯¯ä¿¡æ¯: {str(e)}\n")

        print("=" * 100)
        print(f"æ•…éšœç´¢å¼• {index} å¤„ç†å®Œæˆ ({index + 1}/{MAX_PROCESS_COUNT})")
        print("=" * 100)

    print("\n" + "=" * 100)
    print("ğŸ“Š TiDBæœåŠ¡é›†æˆåˆ†æå¤„ç†ç»Ÿè®¡:")
    print(f"âœ… æˆåŠŸåˆ†æ: {successful_analysis_count} ä¸ª")
    print(f"âŒ åˆ†æå¤±è´¥: {failed_analysis_count} ä¸ª")
    print(f"ğŸ¯ æ€»å¤„ç†æ•°: {successful_analysis_count + failed_analysis_count} ä¸ª")
    print(f"ğŸš€ LLMè°ƒç”¨ä¼˜åŒ–: æ¯ä¸ªæ ·æœ¬åªè°ƒç”¨2æ¬¡ï¼ˆService+TiDBä¸€æ¬¡ï¼ŒNodeç»¼åˆä¸€æ¬¡ï¼‰")
    print("=" * 100)
